{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using fairlearn GridSearch with census data\n",
    "\n",
    "This notebook shows how to use `fairlearn` and the Fairness dashboard to generate models for the Census dataset. This dataset is a classification problem - given a range of data about 32,000 individuals, predict whether their annual income is above or below fifty thousand dollars per year.\n",
    "\n",
    "For the purposes of this notebook, we shall treat this as a loan decision problem. We will pretend that the label indicates whether or not each individual repaid a loan in the past. We will use the data to train a model to predict whether previously unseen individuals will repay a loan or not. The assumption is that the model predictions are used to decide whether an individual should be offered a loan.\n",
    "\n",
    "We will first train a fairness-unaware model and show that it leads to unfair decisions under a specific notion of fairness called *demographic parity*. We then mitigate unfairness by applying the `GridSearch` algorithm from `fairlearn` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data set\n",
    "\n",
    "For simplicity, we import the data set from the `shap` package, which contains the data in a cleaned format. We start by importing the various modules we're going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from fairlearn.metrics import DemographicParity\n",
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions.grid_search.simple_quality_metrics import SimpleClassificationQualityMetric\n",
    "from fairlearn.moments import MisclassificationError, DP\n",
    "\n",
    "from sklearn import svm, neighbors, tree\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load and inspect the data from the `shap` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, Y = shap.datasets.adult()\n",
    "X_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat the sex of each individual as a protected attribute (where 0 indicates female and 1 indicates male), and in this particular case we are going separate this attribute out and drop it from the main data. We then perform some standard data preprocessing steps to convert the data into a format suitable for the ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_raw[\"Sex\"]\n",
    "X = X_raw.drop(labels=['Sex'],axis = 1)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled, \n",
    "                                                    Y, \n",
    "                                                    A,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "# Work around indexing bug\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fairness-unaware model\n",
    "\n",
    "To show the effect of `fairlearn` we will first train a standard ML model that does not incorporate fairness For speed of demonstration, we use a simple logistic regression learner from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unmitigated_model = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "unmitigated_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load this model into the Fairness dashboard, and examine how it is unfair (there is a warning about AzureML since we are not yet integrated with that product):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.explain.model.visualize import FairnessDashboard\n",
    "\n",
    "FairnessDashboard([unmitigated_model,unmitigated_model,unmitigated_model], X_test, Y_test.tolist(), pd.DataFrame(A_test).values.tolist(), True, list(X_test.columns), [0, 1], [\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the error rate for the second group is approximately three times that of the first. Since we are deciding whether a given individual gets a loan or not, the opportunity tab is more important and shows that the second group would also be offered loans approximately three times as often.\n",
    "\n",
    "Despite the fact that we removed the feature from the training data, our model still discriminates based on sex. This demonstrates that simply ignoring a protected attribute when fitting a model rarely eliminates unfairness. There will generally be enough other features correlated with the removed attribute to lead to disparate impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation with GridSearch\n",
    "\n",
    "The `GridSearch` class in `fairlearn` implements a simplified version of the exponentiated gradient reduction of [Agarwal et al. 2018](https://arxiv.org/abs/1803.02453). The user supplies a standard ML learner, which is treated as a blackbox. `GridSearch` works by generating a sequence of relabellings and reweightings, and trains a model for each.\n",
    "\n",
    "For this example, we specify demographic parity (on the protected attribute of sex) as the fairness metric. Demographic parity requires that individuals are offered the opportunity (are approved for a loan in this example) independent of membership in the protected class (i.e., females and males should be offered loans at the same rate). We are using this metric for the sake of simplicity; in general, the appropriate fairness metric will not be obvious.\n",
    "\n",
    "The third argument (quality metric) is not relevant in our example. In general, it is used to select one among the models generated by grid search; however, we will instead examine all the models since they allow us to trace the Pareto curve of trade-offs between fairness and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   disparity_metric=DemographicParity(),\n",
    "                   quality_metric=SimpleClassificationQualityMetric())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithms provide `fit()` and `predict()` methods, so they behave in a similar manner to other ML packages in Python. We do however have to specify two extra arguments to `fit()` - the column of protected attribute labels, and also the number of models to generate in our sweep.\n",
    "\n",
    "After `fit()` completes, we extract the full set of models from the `GridSearch` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep.fit(X_train, Y_train,\n",
    "          aux_data=A_train,\n",
    "          number_of_lagrange_multipliers=71)\n",
    "\n",
    "models = [ z.model for z in sweep.all_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could load these models into the Fairness dashboard now. However, the plot would be somewhat confusing due to their number. In this case, we are going to remove the models which are dominated in the error-disparity space by others from the sweep (note that the disparity will only be calculated for the protected attribute; other potentially protected attributes will not be mitigated). In general, one might not want to do this, since there may be other considerations beyond the strict optimisation of error and disparity (of the given protected attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, disparities = [], []\n",
    "for m in models:\n",
    "    classifier = lambda X: m.predict(X)\n",
    "    \n",
    "    error = MisclassificationError()\n",
    "    error.init(X_train, A_train, pd.Series(Y_train))\n",
    "    disparity = DP()\n",
    "    disparity.init(X_train, A_train, pd.Series(Y_train))\n",
    "    \n",
    "    errors.append(error.gamma(classifier)[0])\n",
    "    disparities.append(disparity.gamma(classifier).max())\n",
    "    \n",
    "all_results = pd.DataFrame( {\"model\": models, \"error\": errors, \"disparity\": disparities})\n",
    "\n",
    "non_dominated = []\n",
    "for row in all_results.itertuples():\n",
    "    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"]<=row.disparity]\n",
    "    if row.error <= errors_for_lower_or_eq_disparity.min():\n",
    "        non_dominated.append(row.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put the non-dominated into the Fairness dashboard. We also add in the original, unmitigated model - it will be the one highlighted when the dashboard first loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_models = [unmitigated_model]\n",
    "dashboard_models.extend(non_dominated)\n",
    "\n",
    "\n",
    "FairnessDashboard(dashboard_models, X_test, Y_test.tolist(), pd.DataFrame(A_test).values.tolist(), True, list(X_test.columns), [0, 1], [\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a Pareto front forming - the set of models which represent optimal tradeoffs between error and disparity. In the ideal case, we would have a model at the origin - perfectly accurate and without any unfairness under demographic parity (with respect to the protected attribute \"sex\"). The Pareto front represents the closest we can come to this ideal based on our data and choice of learner. Note the range of the axes - the disparity axis covers more values than the error, so we can reduce disparity substantially with a smaller increase in error.\n",
    "\n",
    "When we choose a different feature to assess the models on the \"Model Comparison\" tab (for example race), we can see that the smooth tradeoff disappears, with some of the models which are on the Pareto front for mitigation on the basis of sex becoming obviously sub-optimal.\n",
    "\n",
    "By selecting multiple models in the \"Model Comparison\" tab and then changing to the \"Fairness in Accuracy\" tab, we can compare the performance of different models for each of the two classes. This comparison can be performed for any of the available attributes.\n",
    "\n",
    "Finally, we can examine the \"Fairness in Opportunity\" tab. We can see that selecting fairer models is indeed equalising the opportunity (in this toy model, the frequency at which loans are offered) between the classes. We can also see that this is happening by decreasing the acceptance rate for the second group of the protected attribtue.\n",
    "\n",
    "The user can then decide which model on this frontier best suits their required tradeoff between fairness and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
