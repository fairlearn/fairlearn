{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_constrained_optimization as tfco\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import urllib3\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from six.moves import xrange\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "# tfds.disable_progress_bar()\n",
    "\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm, neighbors, tree\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "from fairlearn.reductions import ExponentiatedGradient\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "import statistics\n",
    "\n",
    "from fairlearn.reductions import GridSearch, DemographicParity, ErrorRate, TruePositiveRateParity\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass', 'education', 'marital_status', 'occupation', 'relationship',\n",
    "    'race', 'gender', 'native_country'\n",
    "]\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'age', 'capital_gain', 'capital_loss', 'hours_per_week', 'education_num'\n",
    "]\n",
    "COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "LABEL_COLUMN = 'label'\n",
    "\n",
    "PROTECTED_COLUMNS = [\n",
    "    'gender_Female'\n",
    "]\n",
    "\n",
    "def get_data():\n",
    "    train_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", names=COLUMNS, skipinitialspace=True)\n",
    "    test_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "\n",
    "    train_df_raw[LABEL_COLUMN] = (train_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    test_df_raw[LABEL_COLUMN] = (test_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    # Preprocessing Features\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Functions for preprocessing categorical and continuous columns.\n",
    "    def binarize_categorical_columns(input_train_df, input_test_df, categorical_columns=[]):\n",
    "\n",
    "        def fix_columns(input_train_df, input_test_df):\n",
    "            test_df_missing_cols = set(input_train_df.columns) - set(input_test_df.columns)\n",
    "            for c in test_df_missing_cols:\n",
    "                input_test_df[c] = 0\n",
    "                train_df_missing_cols = set(input_test_df.columns) - set(input_train_df.columns)\n",
    "            for c in train_df_missing_cols:\n",
    "                input_train_df[c] = 0\n",
    "                input_train_df = input_train_df[input_test_df.columns]\n",
    "            return input_train_df, input_test_df\n",
    "\n",
    "        # Binarize categorical columns.\n",
    "        binarized_train_df = pd.get_dummies(input_train_df, columns=categorical_columns)\n",
    "        binarized_test_df = pd.get_dummies(input_test_df, columns=categorical_columns)\n",
    "        # Make sure the train and test dataframes have the same binarized columns.\n",
    "        fixed_train_df, fixed_test_df = fix_columns(binarized_train_df, binarized_test_df)\n",
    "        return fixed_train_df, fixed_test_df\n",
    "\n",
    "    def bucketize_continuous_column(input_train_df,\n",
    "                                  input_test_df,\n",
    "                                  continuous_column_name,\n",
    "                                  num_quantiles=None,\n",
    "                                  bins=None):\n",
    "        assert (num_quantiles is None or bins is None)\n",
    "        if num_quantiles is not None:\n",
    "            train_quantized, bins_quantized = pd.qcut(\n",
    "              input_train_df[continuous_column_name],\n",
    "              num_quantiles,\n",
    "              retbins=True,\n",
    "              labels=False)\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins_quantized, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins_quantized, labels=False)\n",
    "        elif bins is not None:\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins, labels=False)\n",
    "\n",
    "    # Filter out all columns except the ones specified.\n",
    "    train_df = train_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    test_df = test_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    \n",
    "    # Bucketize continuous columns.\n",
    "    bucketize_continuous_column(train_df, test_df, 'age', num_quantiles=4)\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_gain', bins=[-1, 1, 4000, 10000, 100000])\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_loss', bins=[-1, 1, 1800, 1950, 4500])\n",
    "    bucketize_continuous_column(train_df, test_df, 'hours_per_week', bins=[0, 39, 41, 50, 100])\n",
    "    bucketize_continuous_column(train_df, test_df, 'education_num', bins=[0, 8, 9, 11, 16])\n",
    "    train_df, test_df = binarize_categorical_columns(train_df, test_df, categorical_columns=CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS)\n",
    "    feature_names = list(train_df.keys())\n",
    "    feature_names.remove(LABEL_COLUMN)\n",
    "    num_features = len(feature_names)\n",
    "    \n",
    "    return train_df, test_df, feature_names\n",
    "train_df, test_df, FEATURE_NAMES = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "class BuildFunction:\n",
    "    def __init__(self, D):\n",
    "        self.D = D\n",
    "        \n",
    "    def __call__(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, input_dim=D, activation='relu'))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\n",
    "                                                                        tf.keras.metrics.Accuracy(), \n",
    "                                                                        tf.keras.metrics.Recall(),\n",
    "                                                                        tf.keras.metrics.TruePositives(),\n",
    "                                                                        tf.keras.metrics.Precision()\n",
    "                                                                        ])\n",
    "        return model\n",
    "        \n",
    "def create_model_creator(D):\n",
    "    def create_keras_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, input_dim=D, activation='relu'))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\n",
    "                                                                        tf.keras.metrics.Accuracy(), \n",
    "                                                                        tf.keras.metrics.Recall(),\n",
    "                                                                        tf.keras.metrics.TruePositives(),\n",
    "                                                                        tf.keras.metrics.Precision()\n",
    "                                                                        ])\n",
    "        return model\n",
    "    return create_keras_model\n",
    "\n",
    "class LogRegTF(Model):\n",
    "    def __init__(self, D):\n",
    "        super(LogRegTF, self).__init__()\n",
    "        self.W = tf.Variable(tf.random.normal(shape=[D, 1]))\n",
    "        self.b = tf.Variable(tf.random.normal(shape=[1, 1]))\n",
    "\n",
    "    def call(self, x):\n",
    "        print((tf.matmul(x, self.W) + self.b).shape)\n",
    "        return tf.matmul(x, self.W) + self.b\n",
    "\n",
    "class BuildFunctionLogReg:\n",
    "    def __init__(self, D, opt='adam'):\n",
    "        self.D = D\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1, input_dim=D, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=self.opt, metrics=[\n",
    "                                                                        tf.keras.metrics.Accuracy(), \n",
    "                                                                        tf.keras.metrics.Recall(),\n",
    "                                                                        tf.keras.metrics.TruePositives(),\n",
    "                                                                        tf.keras.metrics.Precision()\n",
    "                                                                        ])\n",
    "        return model\n",
    "\n",
    "def fetch_sk_model(name, input_dim, optimizer='adam'):\n",
    "    if name=='mlp':\n",
    "        return MLPClassifier(hidden_layer_sizes=(100, 20), verbose=True, max_iter=10000, solver=optimizer, learning_rate_init=3e-3)\n",
    "    elif name=='nn-keras':\n",
    "        return tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "            build_fn=BuildFunction(input_dim), verbose=False)\n",
    "    elif name=='logistic':\n",
    "        return LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "    elif name=='logistic-keras':\n",
    "        return tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "            build_fn=BuildFunctionLogReg(input_dim, opt=optimizer), verbose=False)\n",
    "    elif name=='logistic-sgd':\n",
    "        return SGDClassifier(loss='log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(FEATURE_NAMES)\n",
    "unmit_nn = fetch_sk_model('logistic', D)\n",
    "unmit_nn.fit(train_df[FEATURE_NAMES].to_numpy(), train_df[LABEL_COLUMN].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, y_train_all, A_train_all = train_df[FEATURE_NAMES], train_df[LABEL_COLUMN], train_df[PROTECTED_COLUMNS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_expgrad(model, X_tr, A_tr, y_tr, constraint=TruePositiveRateParity, \n",
    "                X_te=None, A_te=None, y_te=None, eps=0.01, nu=1e-6,\n",
    "                max_iter=50,\n",
    "                eta0=2.0):\n",
    "    expgrad = ExponentiatedGradient(\n",
    "        model,\n",
    "        constraints=constraint(), eps=eps, nu=nu,\n",
    "        eta0=eta0,\n",
    "        max_iter=max_iter\n",
    "    )\n",
    "    if getattr(model, \"summary\", None) is not None:\n",
    "        model.summary()\n",
    "    else:\n",
    "        print(f\"Model used: {model!r}\")\n",
    "    a=datetime.now()\n",
    "    expgrad.fit(X_tr, y_tr,\n",
    "                           sensitive_features=A_tr)\n",
    "    b=datetime.now()\n",
    "    time_expgrad_all = (b-a).seconds\n",
    "    \n",
    "    def Qexp_all(X): return expgrad._pmf_predict(X)[:, 1]\n",
    "    \n",
    "    # violation of log res\n",
    "    disparity_moment = constraint()\n",
    "    disparity_moment.load_data(X_tr, y_tr,\n",
    "                               sensitive_features=A_tr)\n",
    "    violation_expgrad_all = disparity_moment.gamma(Qexp_all).max()\n",
    "\n",
    "    # error of log res\n",
    "    error = ErrorRate()\n",
    "    error.load_data(X_tr, y_tr,\n",
    "                    sensitive_features=A_tr)\n",
    "    error_expgrad_all = error.gamma(Qexp_all)[0]\n",
    "\n",
    "    print('[TRAINING] Exponentiated gradient : Time: {} seconds; Violation: {}; Error: {}'.format(\n",
    "        time_expgrad_all, violation_expgrad_all, error_expgrad_all))\n",
    "    \n",
    "    if X_te is not None and y_te is not None and A_te is not None:\n",
    "        # violation of log res\n",
    "        disparity_moment = constraint()\n",
    "        disparity_moment.load_data(X_te, y_te,\n",
    "                                   sensitive_features=A_te)\n",
    "        te_violation_expgrad_all = disparity_moment.gamma(Qexp_all).max()\n",
    "\n",
    "        # error of log res\n",
    "        error = ErrorRate()\n",
    "        error.load_data(X_te, y_te,\n",
    "                        sensitive_features=A_te)\n",
    "        te_error_expgrad_all = error.gamma(Qexp_all)[0]\n",
    "\n",
    "        print('[TEST] Exponentiated gradient : Violation: {}; Error: {}'.format(te_violation_expgrad_all, \n",
    "                                                                           te_error_expgrad_all))\n",
    "    return expgrad\n",
    "\n",
    "def run_unmitigated(model, X_tr, A_tr, y_tr, constraint=TruePositiveRateParity, name='None'):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    def Qexp_all(X): return model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # violation of log res\n",
    "    disparity_moment = constraint()\n",
    "    disparity_moment.load_data(X_tr, y_tr,\n",
    "                               sensitive_features=A_tr)\n",
    "    violation_expgrad_all = disparity_moment.gamma(Qexp_all).max()\n",
    "\n",
    "    # error of log res\n",
    "    error = ErrorRate()\n",
    "    error.load_data(X_tr, y_tr,\n",
    "                    sensitive_features=A_tr)\n",
    "    error_expgrad_all = error.gamma(Qexp_all)[0]\n",
    "\n",
    "    print('[{}] Exponentiated gradient : Violation: {}; Error: {}'.format(\n",
    "        name, violation_expgrad_all, error_expgrad_all))\n",
    "    return model\n",
    "\n",
    "# run_expgrad(fetch_sk_model('logistic', D), X_train_all, A_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmitigated run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments\n",
    "print('\\n Logistic Regression with sklearn')\n",
    "expgrad_logistic = run_expgrad(fetch_sk_model('logistic-sgd', D), X_train_all, A_train_all, y_train_all)\n",
    "run_unmitigated(fetch_sk_model('logistic-sgd', D), X_train_all, A_train_all, y_train_all, name='Unmitigated LogReg sklearn')\n",
    "\n",
    "display(Markdown('***'))\n",
    "print('Logistic Regression with Keras')\n",
    "expgrad_logistic_keras = run_expgrad(fetch_sk_model('logistic-keras', D, optimizer='sgd'), X_train_all, A_train_all, y_train_all)\n",
    "run_unmitigated(fetch_sk_model('logistic-keras', D, optimizer='sgd'), X_train_all, A_train_all, y_train_all, name='Unmitigated LogReg keras')\n",
    "\n",
    "display(Markdown('***'))\n",
    "print('Neural Network with Keras')\n",
    "expgrad_nn_keras = run_expgrad(fetch_sk_model('nn-keras', D, optimizer='sgd'), X_train_all, A_train_all, y_train_all)\n",
    "run_unmitigated(fetch_sk_model('nn-keras', D, optimizer='sgd'), X_train_all, A_train_all, y_train_all, name='Unimitigated NN keras')\n",
    "\n",
    "display(Markdown('***'))\n",
    "print('\\n Neural Network with sklearn')\n",
    "expgrad_nn_keras = run_expgrad(fetch_sk_model('mlp', D), X_train_all, A_train_all, y_train_all)\n",
    "run_unmitigated(fetch_sk_model('mlp', D, optimizer='sgd'), X_train_all, A_train_all, y_train_all, name='Unimitigated NN sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
