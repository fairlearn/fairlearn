{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load the data using the tempeh package\n",
    "from tempeh.configurations import datasets\n",
    "dataset = datasets['lawschool_passbar']()\n",
    "\n",
    "X_train, X_test = dataset.get_X(format=pd.DataFrame)\n",
    "y_train, y_test = dataset.get_y(format=pd.Series)\n",
    "A_train, A_test = dataset.get_sensitive_features(name='race', format=pd.Series)\n",
    "\n",
    "# Combine all training data into a single data frame and glance at a few rows\n",
    "all_train = pd.concat([X_train, y_train, A_train], axis=1)\n",
    "display(all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_grouped = all_train.groupby('race')\n",
    "\n",
    "counts_by_race = all_train_grouped[['lsat']].count().rename(\n",
    "    columns={'lsat': 'count'})\n",
    "\n",
    "quartiles_by_race = all_train_grouped[['lsat','ugpa']].quantile([.25, .50, .75]).rename(\n",
    "    index={0.25: \"25%\", 0.5: \"50%\", 0.75: \"75%\"}, level=1).unstack()\n",
    "\n",
    "rates_by_race = all_train_grouped[['pass_bar']].mean().rename(\n",
    "    columns={'pass_bar': 'pass_bar_rate'})\n",
    "\n",
    "summary_by_race = pd.concat([counts_by_race, quartiles_by_race, rates_by_race], axis=1)\n",
    "display(summary_by_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmitigated Predictor\n",
    "\n",
    "We first train a standard logistic regression predictor that does not seek to incorporate any notion of fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "unmitigated_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We view the probabilistic predictions produced by the logistic model as scores and evaluate the quality of the ranking they produce in terms of the area under the ROC curve (AUC). AUC is equal to the probability that a randomly chosen positive example (i.e., a student who passes the bar) is scored above a randomly chosen negative example (i.e., a student who does not pass the bar). An AUC of 0.5 means that the scores are no better than a random coin flip, whereas AUC of 1.0 means that the scores perfectly separate positives from negatives. The AUC metric has two desirable properties: (1) it is preserved by monotone transformations of the score, and (2) it is not sensitive to the imbalance between positives and negatives, which is quite severe in our example, with the overall bar passage rate above 94%.\n",
    "\n",
    "Note that the logistic regression estimator above does not seek to optimize AUC directly, but only seeks to optimize the logistic loss. However, a good logistic loss is also expected to yield a good AUC.\n",
    "\n",
    "To obtain the AUC values for the overall student population as well as black and white subpopulations, we use the group metric variant of the sklearn metric roc_auc_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_roc_auc_score\n",
    "\n",
    "# a convenience function that transforms the result of a group metric call into a data frame\n",
    "def group_metric_as_df(name, group_metric_result):\n",
    "    a = pd.Series(group_metric_result.by_group)\n",
    "    a['overall'] = group_metric_result.overall\n",
    "    return pd.DataFrame({name: a})\n",
    "\n",
    "scores_unmitigated = pd.Series(unmitigated_predictor.predict_proba(X_test)[:,1], name=\"score_unmitigated\")\n",
    "auc_unmitigated = group_metric_as_df(\"auc_unmitigated\",\n",
    "                                     group_roc_auc_score(y_test, scores_unmitigated, A_test))\n",
    "\n",
    "display(HTML('<span id=\"auc_unmitigated\">'),\n",
    "        auc_unmitigated,\n",
    "        HTML('</span>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(y,predicted_y):\n",
    "    correct_y = (y == predicted_y)\n",
    "    return 1 - sum(correct_y)/len(correct_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_y = pd.Series(unmitigated_predictor.predict(X_test),name=\"unmitigated_predicted_y\")\n",
    "error_unmitigated = [get_error(y_test,unmitigated_y)]\n",
    "print(\"The error for unmitigated is:\")\n",
    "print(error_unmitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the violation of the fairness constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_violation(predict_y,A_test,label_name):\n",
    "    violations = []\n",
    "    predicted_and_sensitiveFeature = pd.concat([predict_y,A_test],axis=1)\n",
    "    grouped =predicted_and_sensitiveFeature.groupby('race')\n",
    "    counts_by_race = grouped[[label_name]].count()\n",
    "    passed_by_race = grouped[[label_name]].sum()\n",
    "#     display(counts_by_race)\n",
    "#     display(passed_by_race)\n",
    "    for i,group in enumerate(grouped.groups.keys()):\n",
    "        violation_1 = passed_by_race[label_name][i] / counts_by_race[label_name][i]\n",
    "        violation_2 = sum(predict_y) / len(predict_y)\n",
    "        violations.append(abs(violation_1 - violation_2))\n",
    "    violation = max(violations)\n",
    "    return violation\n",
    "    \n",
    "violation_unmitigated = [get_violation(unmitigated_y,A_test,'unmitigated_predicted_y')]\n",
    "print(\"The violation for unmitigated is:\")\n",
    "print(violation_unmitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient\n",
    "from fairlearn.reductions import GridSearch, DemographicParity\n",
    "import numpy as np\n",
    "# eps_list = [0.001,0.01,0.1]\n",
    "eps_list = list(np.arange(0.001, 0.1, 0.001))\n",
    "expgrad_error = []\n",
    "expgrad_violation = []\n",
    "balanced_index_pass0 = y_train[y_train==0].index \n",
    "balanced_index_pass1 = y_train[y_train==1].sample(n=balanced_index_pass0.size, random_state=0).index\n",
    "balanced_index = balanced_index_pass0.union(balanced_index_pass1)\n",
    "\n",
    "for eps in eps_list:\n",
    "    expgrad_X = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=eps,\n",
    "    nu=1e-6)\n",
    "    \n",
    "    expgrad_X.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    sensitive_features=A_train)\n",
    "    \n",
    "    expgrad_y = pd.Series(expgrad_X.predict(X_test),name=\"expgrad_predicted_y\")\n",
    "    error_expgrad = get_error(y_test,expgrad_y)\n",
    "    expgrad_error.append(error_expgrad)\n",
    "    violation_expgrad = get_violation(expgrad_y,A_test,\"expgrad_predicted_y\")\n",
    "    expgrad_violation.append(violation_expgrad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(expgrad_violation,expgrad_error,label=\"expgrad\")\n",
    "plt.plot(violation_unmitigated,error_unmitigated,'ro',label=\"unmitigated\")\n",
    "plt.xlabel('Violation of the fairness constraint')\n",
    "plt.ylabel('Error')\n",
    "plt.ylim(ymin = 0.04783, ymax = 0.04845)\n",
    "plt.xlim(xmin = 0.0058, xmax = 0.0161)\n",
    "plt.title('Law Schools/DP/log.reg')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
