{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fairlearn with Census Data\n",
    "\n",
    "This notebook shows how to use `fairlearn` and the Fairness dashboard to generate models for the Census dataset. This dataset is a classification problem - given a range of data about 32,000 individuals, predict whether their annual income is above or below fifty thousand dollars per year.\n",
    "\n",
    "For simplicity, we import the dataset from the `shap` package, which contains the data in a cleaned format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from fairlearn.metrics import DemographicParity\n",
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions.grid_search.simple_quality_metrics import SimpleClassificationQualityMetric\n",
    "\n",
    "from sklearn import svm, neighbors, tree\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "shap.initjs()\n",
    "X_raw,y = shap.datasets.adult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the raw data, seeing the available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat the gender of each individual as a protected attribute, so we separate it out and drop it from the main data. We use `get_dummies` to convert any categorial columns to indicator variables, and then ensure that the data are scaled to similar magnitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_raw[\"Sex\"]\n",
    "X = X_raw.drop(labels=['Sex'],axis = 1)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the supplied labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These need to be converted to indicator values as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform the normal split of the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(X_scaled, \n",
    "                                                    y, \n",
    "                                                    A,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now a slightly subtlety, which is not currently addressed in `fairlearn` itself (yet). Note that `y` is a Python list, but that `X_scaled` and `A` were pandas DataFrames. In the `train_test_split` method above, Python lists are simply split but DataFrames are instead split as a list of row indices (thereby avoiding the need to copy the data). Inside `fairlearn` we combine `a_train` and `y_train` into a new DataFrame with one column for each variable (both will have the same number of elements). When pandas does this, it sees that `a_train` is a DataFrame with indices, and it uses those as the basis for the new column size. The indices correspond to the original, unsplit, data, so this makes that column much longer than the `y_train` column; pandas fills the missing values automatically, but the rest of `fairlearn` subsequently fails due to the combined DataFrame having many more rows than expected.\n",
    "\n",
    "To avoid this, we take copy `x_train` and `a_train` into new DataFrames, with the indices reset to be sequential (and dropping the old indices entirely):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reset_index(drop=True)\n",
    "a_train = a_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our grid search. We use a simple logistic regression classifier in the interests of speed. We also specify that our equality goal is demographic parity. The quality metric seeks to maximise the sum of accuracy and parity (as measured by demographic parity). This quality metric then allows the `GridSearch` object to select a model to use for `predict` calls. In this case, though, we just extract the generated models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   fairness_metric=DemographicParity(),\n",
    "                   quality_metric=SimpleClassificationQualityMetric())\n",
    "\n",
    "sweep.fit(x_train, y_train,\n",
    "          protected_attribute=a_train,\n",
    "          number_of_lagrange_multipliers=71)\n",
    "\n",
    "models = [ x[\"model\"] for x in sweep.all_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at the models in the dashboard. First we import the code (since we do not yet have integration with AzureML, there is a warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.explain.model.visualize import FairnessDashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can open the visualisation. We can see a clear Pareto front forming. Individual models can be selected for further exploration on the other tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FairnessDashboard(models, x_test, y_test.tolist(), pd.DataFrame(a_test).values.tolist(), True, list(x_test.columns), [0, 1], [\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
