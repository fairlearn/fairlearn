{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and preparing the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the post processing algorithm we use the \"Adult Data Set\" from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/adult). The task there is to predict whether a person makes more or less than $50k based on just a few attributes such as age, sex, race, education, occupation, etc.\n",
    "For the purposes of this notebook, we can interpret this as a loan decision problem. The label indicates whether or not each individual repaid a loan in the past. We will use the data to train a model to predict whether previously unseen individuals will repay a loan or not. The assumption is that the model predictions are used to decide whether an individual should be offered a loan.\n",
    "\n",
    "To start, let's download the dataset using `shap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following command to add the parent directory to the path is necessary to make Python discover the fairlearn code\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "X, y = shap.datasets.adult()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to encode categorical attributes properly, there are a few additional steps to take. We'll also split the data into train and test sets. The attribute we're interested in for the purpose of this notebook is `Sex`. Female is encoded as 0 and male as 1. Note: our approach works even for more than two groups, but the dataset didn't provide that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "A = X[\"Sex\"]\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, \n",
    "                                                                     y, \n",
    "                                                                     A,\n",
    "                                                                     test_size = 0.2,\n",
    "                                                                     random_state=0,\n",
    "                                                                     stratify=y)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a fairness-unaware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "fairness_unaware_model = LogisticRegression(solver='liblinear')\n",
    "fairness_unaware_model.fit(X_train, y_train)\n",
    "\n",
    "def show_proportions(X, y_pred, y=None, description=None):\n",
    "    male_indices = X.index[X.Sex == 1]\n",
    "    female_indices = X.index[X.Sex == 0]\n",
    "\n",
    "    males_earning_more_than_50k = sum(y_pred[male_indices])\n",
    "    females_earning_more_than_50k = sum(y_pred[female_indices])\n",
    "    print(\"\\n\" + description)\n",
    "    print(\"P[loan approval | male] = {}\".format(males_earning_more_than_50k/len(male_indices)))\n",
    "    print(\"P[loan approval | female] = {}\".format(females_earning_more_than_50k/len(female_indices)))\n",
    "    \n",
    "    if y is not None:\n",
    "        positive_male_indices = X.index[(X.Sex == 1) & (y == 1)]\n",
    "        negative_male_indices = X.index[(X.Sex == 1) & (y == 0)]\n",
    "        positive_female_indices = X.index[(X.Sex == 0) & (y == 1)]\n",
    "        negative_female_indices = X.index[(X.Sex == 0) & (y == 0)]\n",
    "        print(\"P[loan approval | male, loan repaid] = {}\".format(sum(y_pred[positive_male_indices])/len(positive_male_indices)))\n",
    "        print(\"P[loan approval | female, loan repaid] = {}\".format(sum(y_pred[positive_female_indices])/len(positive_female_indices)))\n",
    "        print(\"P[loan approval | male, loan not repaid] = {}\".format(sum(y_pred[negative_male_indices])/len(negative_male_indices)))\n",
    "        print(\"P[loan approval | female, loan not repaid] = {}\".format(sum(y_pred[negative_female_indices])/len(negative_female_indices)))\n",
    "    \n",
    "show_proportions(X_train, y_train, description=\"original training data:\")\n",
    "show_proportions(X_train, fairness_unaware_model.predict(X_train), y_train, description=\"fairness-unaware prediction on training data:\")\n",
    "show_proportions(X_test, y_test, description=\"original test data:\")\n",
    "show_proportions(X_test, fairness_unaware_model.predict(X_test), y_test, description=\"fairness-unaware prediction on test data:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a stark contrast in the predictions with males being a lot more likely to be predicted to get approved, similar to the original training data. However, there's even a disparity between the subgroup of males and females that are approved with 54.7% of males predicted to get approved, and only 42.4% of females. When considering only the samples labeled with \"loan not repaid\" males (9.8%) are more than five times as likely as females (1.4%) to be predicted to get approved. The test data shows a similar disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.explain.model.visualize import FairnessDashboard\n",
    "\n",
    "FairnessDashboard([fairness_unaware_model,fairness_unaware_model,fairness_unaware_model], X_test, y_test.tolist(), pd.DataFrame(A_test).values.tolist(), True, list(X_test.columns), [0, 1], [\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing the model to get a fair model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind post-processing is to alter the output of the fairness-unaware model to achieve fairness. The post-processing algorithm requires three input arguments:\n",
    "- the matrix of samples X\n",
    "- the vector of predictions y from the fairness-unaware model \n",
    "- the vector of protected attribute values A\n",
    "\n",
    "The goal is to make the output fair with respect to a disparity metric. Our post-processing algorithm uses one of\n",
    "- Demographic Parity (DP): $P[h(X)=\\hat{y} | A=a] = P[h(X)=\\hat{y}] \\qquad \\forall a, \\hat{y}$\n",
    "- Equalized Odds (EO): $P[h(X)=\\hat{y} | A=a, Y=y] = P[h(X)=\\hat{y}|Y=y] \\qquad \\forall a, \\hat{y}$\n",
    "\n",
    "where $h(X)$ is the prediction based on the input $X$, $\\hat{y}$ and $y$ are labels, and $a$ is a protected attribute value. In our example, we'd expect the post-processed model with DP to be balanced between sexes. EO does not make the same guarantees. Instead, it ensures that the parity between the subgroups of each sex with label 1 in the training set, and parity between the subgroups of each sex with label 0 in the training set. Applied to our scenario, this means that men and women with who have repaid their loan in the past are equally likely to be approved for a new loan (and therefore also equally likely to be rejected). Similarly, there is parity between men and women who have not repaid a loan, but we have no parity between the groups with different training labels. In mathematical terms:\n",
    "\n",
    "$$\n",
    "P[\\text{loan approval} | \\text{male, loan repaid}] = P[\\text{loan approval} | \\text{female, loan repaid}], \\text{e.g. } 0.95\\\\\n",
    "P[\\text{loan approval} | \\text{male, loan not repaid}] = P[\\text{loan approval} | \\text{female, loan not repaid}], \\text{e.g. } 0.15\n",
    "$$\n",
    "\n",
    "but that also means that men (and women) of different subgroup based on training labels don't necessarily have parity:\n",
    "\n",
    "$$\n",
    "P[\\text{loan approval} | \\text{male, loan repaid}] = 0.95 \\neq 0.15 = P[\\text{loan approval} | \\text{male, loan not repaid}]\n",
    "$$\n",
    "\n",
    "Assessing which disparity metric is indeed fair varies by application scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing import ROCCurveBasedPostProcessing\n",
    "from copy import deepcopy\n",
    "\n",
    "post_processed_model_DP = ROCCurveBasedPostProcessing(fairness_unaware_model=deepcopy(fairness_unaware_model), disparity_metric=\"DemographicParity\", plot=True, seed=0)\n",
    "\n",
    "post_processed_model_DP.fit(X_train, y_train, aux_data=X_train.Sex)\n",
    "\n",
    "fairness_aware_predictions_DP_train = post_processed_model_DP.predict(X_train, group_data=X_train.Sex)\n",
    "fairness_aware_predictions_DP_test = post_processed_model_DP.predict(X_test, group_data=X_test.Sex)\n",
    "\n",
    "show_proportions(X_train, fairness_aware_predictions_DP_train, y_train, description=\"demographic parity with post-processed model on training data:\")\n",
    "show_proportions(X_test, fairness_aware_predictions_DP_test, y_test, description=\"demographic parity with post-processed model on test data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing import ROCCurveBasedPostProcessing\n",
    "from copy import deepcopy\n",
    "\n",
    "post_processed_model_EO = ROCCurveBasedPostProcessing(fairness_unaware_model=deepcopy(fairness_unaware_model), disparity_metric=\"EqualizedOdds\", plot=True, seed=0)\n",
    "\n",
    "post_processed_model_EO.fit(X_train, y_train, aux_data=X_train.Sex)\n",
    "\n",
    "fairness_aware_predictions_EO_train = post_processed_model_EO.predict(X_train, group_data=X_train.Sex)\n",
    "fairness_aware_predictions_EO_test = post_processed_model_EO.predict(X_test, group_data=X_test.Sex)\n",
    "\n",
    "show_proportions(X_train, fairness_aware_predictions_EO_train, y_train, description=\"equalized odds with post-processed model on training data:\")\n",
    "show_proportions(X_test, fairness_aware_predictions_EO_test, y_test, description=\"equalized odds with post-processed model on test data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget doesn't work with post processed models yet due to additional argument A\n",
    "# FairnessDashboard([post_processed_model_DP,post_processed_model_DP,post_processed_model_EO], X_test, y_test.tolist(), pd.DataFrame(A_test).values.tolist(), True, list(X_test.columns), [0, 1], [\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing in Detail\n",
    "\n",
    "While this worked as the numbers show, it's not entirely obvious how it found its solution. The following section will provide a deep dive on post processing for both Demographic Parity (DP) and Equalized Odds (EO).\n",
    "\n",
    "Our post processing method (based on work by [Hardt, Price, Srebro](https://arxiv.org/pdf/1610.02413.pdf)) takes a fairness-unaware model and a disparity metric (such as DP, or EO) in the constructor and features (X), labels (y), and auxiliary data (aux_data) in the fit method. It subsequently uses the model to make predictions for all samples in X. Note that these predictions could be 0/1 (as in our example), or more categories, or even real valued scores.\n",
    "In our case, this looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = fairness_unaware_model.predict(X_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding threshold rules\n",
    "\n",
    "The algorithm then tries to find all thresholding rules with which it could divide the data. Any score for which the thresholding rule evaluates to true is predicted to be 1. It does that for each group, so in this case once for male and once for female. In this case that's pretty simple since we only have two possible values, 0 and 1, so the thresholding rules should be based on \n",
    "\n",
    "- $<0$ or $>1$ --> label nothing as 1, although the code usually expresses this as \">inf\" (more than infinity) or \"<-inf\"\n",
    "- $\\leq 0$ --> label all 0 scores as 1, although the code expresses this based on the point between 0 and 1, so $\\leq0.5$\n",
    "- $\\geq 0$ --> label everything as 1, although the code usually expresses this as \">-inf\" (more than infinity) or \"<inf\"\n",
    "- $\\geq 1$ --> label all 1 scores as 1, although the code expresses this based on the point between 0 and 1, so $\\geq0.5$\n",
    "\n",
    "Depending on your scores you could have dozens of thresholding rules, between each set of such values. For each rule we just evaluate the following two probabilities empirically:\n",
    "\n",
    "$$\n",
    "P[\\hat{Y} = 1 | Y = 0] \\text{ which is labeled x below to indicate that it'll be plotted on the x-axis}\\\\\n",
    "P[\\hat{Y} = 1 | Y = 1] \\text{ which is labeled y below to indicate that it'll be plotted on the y-axis}\n",
    "$$\n",
    "\n",
    "The former is the probability of misclassifying negative ($Y=0$) labels, while the latter is the probability of correctly classifying positive ($Y=1$) labels. In our example, the threshold rules would be:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing.roc_curve_based_post_processing import _reformat_and_group_data\n",
    "data_grouped_by_attribute = _reformat_and_group_data(A_train, y_train, scores)\n",
    "data_grouped_by_attribute.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_group = data_grouped_by_attribute.get_group(0)\n",
    "male_group = data_grouped_by_attribute.get_group(1)\n",
    "n_female = len(female_group)\n",
    "n_male = len(male_group)\n",
    "n_female, n_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing._roc_curve_utilities import _calculate_roc_points\n",
    "roc_points_female = _calculate_roc_points(female_group, 0)\n",
    "roc_points_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_points_male = _calculate_roc_points(male_group, 0)\n",
    "roc_points_male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base points with (x,y) as (0,0) and (1,1) always exist, because that essentially just means that we're predicting everything as 0 or everything as 1 regardless of the scores from the fairness-unaware model. Let's look at both cases:\n",
    "- x=0, y=0, threshold rule \">inf\": more than infinity is impossible, which means every sample is predicted as 0. That means $P[\\hat{Y} = 1 | Y = 0] = 0$ (represented as x) because our predictions $\\hat(Y)$ are never 1, and similarly $P[\\hat{Y} = 1 | Y = 1] = 0$ (represented as y).\n",
    "- x=1, y=1, threshold rule \">-inf\": more than infinity is always true, which means every sample is predicted as 1. That means $P[\\hat{Y} = 1 | Y = 0] = 1$ (represented as x) because our predictions $\\hat(Y)$ are always 1, and similarly $P[\\hat{Y} = 1 | Y = 1] = 1$ (represented as y).\n",
    "\n",
    "The more interesting logic happens in the third/middle case. Both adopted the thresholding rule $\\geq0.5$. The x and y values were calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_female_1 = sum(female_group[\"label\"])\n",
    "n_female_0 = len(female_group) - n_female_1\n",
    "n_male_1 = sum(male_group[\"label\"])\n",
    "n_male_0 = len(male_group) - n_male_1\n",
    "print(\"Female:\")\n",
    "print(\"    number of samples with label 1: {}\".format(n_female_1))\n",
    "print(\"    number of samples with label 0: {}\".format(n_female_0))\n",
    "print(\"Male:\")\n",
    "print(\"    number of samples with label 1: {}\".format(n_male_1))\n",
    "print(\"    number of samples with label 0: {}\".format(n_male_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_female_0_5 = sum((female_group[\"score\"] > 0.5) & (female_group[\"label\"] == 0)) / n_female_0\n",
    "y_female_0_5 = sum((female_group[\"score\"] > 0.5) & (female_group[\"label\"] == 1)) / n_female_1\n",
    "x_male_0_5 = sum((male_group[\"score\"] > 0.5) & (male_group[\"label\"] == 0)) / n_male_0\n",
    "y_male_0_5 = sum((male_group[\"score\"] > 0.5) & (male_group[\"label\"] == 1)) / n_male_1\n",
    "print(\"Female:\")\n",
    "print(\"    P[Ŷ = 1 | Y = 0] = {}\".format(x_female_0_5))\n",
    "print(\"    P[Ŷ = 1 | Y = 0] = {}\".format(y_female_0_5))\n",
    "print(\"Male:\")\n",
    "print(\"    P[Ŷ = 1 | Y = 0] = {}\".format(x_male_0_5))\n",
    "print(\"    P[Ŷ = 1 | Y = 0] = {}\".format(y_male_0_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it never makes sense to have $x>y$ because in that case you're better off flipping labels, i.e. completely turning around the meaning of the scores. Our method automatically does that unless specified otherwise.\n",
    "\n",
    "## Interpolated Predictions and Probabilistic Classifiers\n",
    "\n",
    "This way you end up with a set of points above the diagonal line connecting (0,0) and (1,1). We calculate the convex hull based on that, because we can reach any point in between two known thresholding points by interpolation. An interpolation could be $p_0 (x_0, y_0) + p_1 (x_1, y_1)$. For our post processing algorithm that would mean that we use the rule defined by $(x_0, y_0, \\text{operation}_0)$ $p_0$ percent of the time, and the rule defined by $(x_1, y_1, \\text{operation}_1)$ $p_1$ percent of the time, thus resulting in a probabilistic classifier. Depending on the data certain fairness objectives can only be accomplished with probabilistic classifiers. However, not every use case lends itself to probabilistic classifiers, since it could mean that two people with identical features are classified differently.\n",
    "\n",
    "## Finding the Equalized Odds solution\n",
    "\n",
    "In this specific instance we don't need to do any extra work to get the convex hull, since there's only one point above the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(roc_points_female.x, roc_points_female.y)\n",
    "plt.xlabel(\"$P[\\\\hat{Y}=1|Y=0]$\")\n",
    "plt.ylabel(\"$P[\\\\hat{Y}=1|Y=1]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(roc_points_male.x, roc_points_male.y)\n",
    "plt.xlabel(\"$P[\\\\hat{Y}=1|Y=0]$\")\n",
    "plt.ylabel(\"$P[\\\\hat{Y}=1|Y=1]$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Equalized Odds case, we need to enforce the presented probabilities to match, which is equivalent to finding the minimum error overlap. The error in the chart is smallest in the top left corner. This is done as part of the `fit` step above, and we'll repeat it here for completeness. The yellow area is the overlap between the areas under the curve that are reachable with interpolation for both groups. Of course, this works for more than two groups as well. The result is that we have interpolated solutions for each group, i.e. every prediction is calculated as the weighted result of two threshold rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_model_EO.fit(X_train, y_train, aux_data=X_train.Sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually even look up the specific interpolations and interpret the results. Keep in mind that these interpolations come up with a floating point number between 0 and 1, and represent the probability of getting 0 or 1 in the predicted outcome.\n",
    "\n",
    "For female samples 95% of the predicted outcome depends on a score above 0.5, while they start with a 5% probability regardless of the score. That means that any female with a score above 0.5 is virtually guaranteed to be approved for a loan.\n",
    "\n",
    "For male samples there's always a 23% chance of not getting the loan regardless of the score, and if their score is below 0.5 they have no chance of approval.\n",
    "\n",
    "Note that this does not necessarily mean it's fair. It simply enforced the constraints we asked it to enforce, as described by Equalized Odds.\n",
    "\n",
    "The parameters `p_ignore` and `prediction_constant` are irrelevant for cases where the curves intersect in the minimum error point. When that doesn't happen, and the minimum error point is only part of one curve, then the interpolation is adjusted as follows\n",
    "```\n",
    "p_ignore * prediction_constant + (1 - p_ignore) * (p0 * operation0(score) + p1 * operation1(score))\n",
    "```\n",
    "The adjustment should happen to the higher one of the curves and essentially brings it closer to the diagonal as represented by `prediction_constant`. In our case this is not required since the curves intersect, but we are actually slightly inaccurate because we only determine the minimum error point on a grid of x values, instead of calculating the intersection point analytically. By choosing a large `gridsize` this can be alleviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, interpolation in post_processed_model_EO._post_processed_model_by_attribute.items():\n",
    "    print(\"female:\" if group == 0 else \"male:\")\n",
    "    print(\"\\n \".join(interpolation.__repr__().split(',')))\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Demographic Parity solution\n",
    "\n",
    "In the Demographic Parity case we can't exactly use the same x and y, since we're not conditioning on the labels in the training data. Instead, we only care about the probabilities of the predicted outcomes. That's why in that case we plot the selection rate (= percentage of predicted 1s) on the x-axis and the error rate on the y-axis. Some of the steps on the way are actually identical. We calculate the convex hull the same way. From the probabilities in that data we can calculate the probability of getting a prediction with label 1:\n",
    "\n",
    "$$\n",
    "P[\\hat{Y} = 1] = \\dfrac{n_1 P[\\hat{Y} = 1 | Y = 1] + n_0 P[\\hat{Y} = 1 | Y = 0]}{n}\n",
    "$$\n",
    "\n",
    "and we calculate the error rate:\n",
    "$$\n",
    "P[\\hat{Y} \\neq Y] = \\dfrac{n_0 P[\\hat{Y} = 1 | Y = 0] + n_1 P[\\hat{Y} = 0 | Y = 1]}{n}\n",
    "$$\n",
    "\n",
    "To show the graph again we'll repeat the `fit` step from above. The chosen solution is where the sum of the errors weighted by the number of samples per group is minimized. In this example we have a lot more men than women in the dataset, which explains why it's choosing the selection rate at the point where it minimizes the error for the male curve.\n",
    "\n",
    "Note that the error is explicitly plotted this time, so lower y-values are preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_model_DP.fit(X_train, y_train, aux_data=X_train.Sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, interpolation in post_processed_model_DP._post_processed_model_by_attribute.items():\n",
    "    print(\"female:\" if group == 0 else \"male:\")\n",
    "    print(\"\\n \".join(interpolation.__repr__().split(',')))\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this solution in a similar way as before. Women have a 19% chance of getting the loan regardless of the score, whereas men have a 0.3% chance. Both men and women with scores over 0.5 are guaranteed to get a loan. Note that this does not necessarily mean it's fair. It simply enforced the constraints we asked it to enforce, as described by Demographic Parity. This increased probability for women balances out the much lower approval rate in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
