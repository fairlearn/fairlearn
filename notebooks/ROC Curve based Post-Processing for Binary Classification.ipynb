{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and preparing the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the post processing algorithm we use the \"Adult Data Set\" from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/adult). The task there is to predict whether a person makes more or less than $50k based on just a few attributes such as age, sex, race, education, occupation, etc.\n",
    "For the purposes of this notebook, we can interpret this as a loan decision problem. The label indicates whether or not each individual repaid a loan in the past. We will use the data to train a model to predict whether previously unseen individuals will repay a loan or not. The assumption is that the model predictions are used to decide whether an individual should be offered a loan.\n",
    "\n",
    "To start, let's download the dataset using `shap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "X, y = shap.datasets.adult()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to encode categorical attributes properly, there are a few additional steps to take. We'll also split the data into train and test sets. The attribute we're interested in for the purpose of this notebook is `Sex`. Female is encoded as 0 and male as 1. Note: our approach works even for more than two groups, but the dataset didn't provide that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "A = X[\"Sex\"]\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, \n",
    "                                                                     y, \n",
    "                                                                     A,\n",
    "                                                                     test_size = 0.2,\n",
    "                                                                     random_state=0,\n",
    "                                                                     stratify=y)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a fairness-unaware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "fairness_unaware_model = LogisticRegression(solver='liblinear')\n",
    "fairness_unaware_model.fit(X_train, y_train)\n",
    "\n",
    "def show_proportions(X, y_pred, y=None, description=None):\n",
    "    male_indices = X.index[X.Sex == 1]\n",
    "    female_indices = X.index[X.Sex == 0]\n",
    "\n",
    "    males_earning_more_than_50k = sum(y_pred[male_indices])\n",
    "    females_earning_more_than_50k = sum(y_pred[female_indices])\n",
    "    print(\"\\n\" + description)\n",
    "    print(\"P[loan approval | male] = {}\".format(males_earning_more_than_50k/len(male_indices)))\n",
    "    print(\"P[loan approval | female] = {}\".format(females_earning_more_than_50k/len(female_indices)))\n",
    "    \n",
    "    if y is not None:\n",
    "        positive_male_indices = X.index[(X.Sex == 1) & (y == 1)]\n",
    "        negative_male_indices = X.index[(X.Sex == 1) & (y == 0)]\n",
    "        positive_female_indices = X.index[(X.Sex == 0) & (y == 1)]\n",
    "        negative_female_indices = X.index[(X.Sex == 0) & (y == 0)]\n",
    "        print(\"P[loan approval | male, loan repaid] = {}\".format(sum(y_pred[positive_male_indices])/len(positive_male_indices)))\n",
    "        print(\"P[loan approval | female, loan repaid] = {}\".format(sum(y_pred[positive_female_indices])/len(positive_female_indices)))\n",
    "        print(\"P[loan approval | male, loan not repaid] = {}\".format(sum(y_pred[negative_male_indices])/len(negative_male_indices)))\n",
    "        print(\"P[loan approval | female, loan not repaid] = {}\".format(sum(y_pred[negative_female_indices])/len(negative_female_indices)))\n",
    "    \n",
    "show_proportions(X_train, y_train, description=\"original training data:\")\n",
    "show_proportions(X_train, fairness_unaware_model.predict(X_train), y_train, description=\"fairness-unaware prediction on training data:\")\n",
    "show_proportions(X_test, y_test, description=\"original test data:\")\n",
    "show_proportions(X_test, fairness_unaware_model.predict(X_test), y_test, description=\"fairness-unaware prediction on test data:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a stark contrast in the predictions with males being a lot more likely to be predicted to get approved, similar to the original training data. However, there's even a disparity between the subgroup of males and females that are approved with 54.7% of males predicted to get approved, and only 42.4% of females. When considering only the samples labeled with \"loan not repaid\" males (9.8%) are more than five times as likely as females (1.4%) to be predicted to get approved. The test data shows a similar disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.explain.model.visualize import FairnessDashboard\n",
    "\n",
    "FairnessDashboard([fairness_unaware_model,fairness_unaware_model,fairness_unaware_model], X_test, y_test.tolist(), pd.DataFrame(A_test).values.tolist(), True, list(X_test.columns), [0, 1], [\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing the model to get a fair model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind post-processing is to alter the output of the fairness-unaware model to achieve fairness. The post-processing algorithm requires three input arguments:\n",
    "- the matrix of samples X\n",
    "- the vector of predictions y from the fairness-unaware model \n",
    "- the vector of protected attribute values A\n",
    "\n",
    "The goal is to make the output fair with respect to a disparity metric. Our post-processing algorithm uses one of\n",
    "- Demographic Parity (DP): $P[h(X)=\\hat{y} | A=a] = P[h(X)=\\hat{y}] \\qquad \\forall a, \\hat{y}$\n",
    "- Equalized Odds (EO): $P[h(X)=\\hat{y} | A=a, Y=y] = P[h(X)=\\hat{y}|Y=y] \\qquad \\forall a, \\hat{y}$\n",
    "\n",
    "where $h(X)$ is the prediction based on the input $X$, $\\hat{y}$ and $y$ are labels, and $a$ is a protected attribute value. In our example, we'd expect the post-processed model with DP to be balanced between sexes. EO does not make the same guarantees. Instead, it ensures that the parity between the subgroups of each sex with label 1 in the training set, and parity between the subgroups of each sex with label 0 in the training set. Applied to our scenario, this means that men and women with who have repaid their loan in the past are equally likely to be approved for a new loan (and therefore also equally likely to be rejected). Similarly, there is parity between men and women who have not repaid a loan, but we have no parity between the groups with different training labels. In mathematical terms:\n",
    "\n",
    "$$\n",
    "P[\\text{loan approval} | \\text{male, loan repaid}] = P[\\text{loan approval} | \\text{female, loan repaid}], \\text{e.g. } 0.95\\\\\n",
    "P[\\text{loan approval} | \\text{male, loan not repaid}] = P[\\text{loan approval} | \\text{female, loan not repaid}], \\text{e.g. } 0.15\n",
    "$$\n",
    "\n",
    "but that also means that men (and women) of different subgroup based on training labels don't necessarily have parity:\n",
    "\n",
    "$$\n",
    "P[\\text{loan approval} | \\text{male, loan repaid}] = 0.95 \\neq 0.15 = P[\\text{loan approval} | \\text{male, loan not repaid}]\n",
    "$$\n",
    "\n",
    "Assessing which disparity metric is indeed fair varies by application scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing import ROCCurveBasedPostProcessing\n",
    "from copy import deepcopy\n",
    "\n",
    "post_processed_model_DP = ROCCurveBasedPostProcessing(fairness_unaware_model=deepcopy(fairness_unaware_model), disparity_metric=\"DemographicParity\", plot=True, seed=0)\n",
    "\n",
    "post_processed_model_DP.fit(X_train, y_train, aux_data=X_train.Sex)\n",
    "\n",
    "fairness_aware_predictions_DP_train = post_processed_model_DP.predict(X_train, group_data=X_train.Sex)\n",
    "fairness_aware_predictions_DP_test = post_processed_model_DP.predict(X_test, group_data=X_test.Sex)\n",
    "\n",
    "show_proportions(X_train, fairness_aware_predictions_DP_train, y_train, description=\"demographic parity with post-processed model on training data:\")\n",
    "show_proportions(X_test, fairness_aware_predictions_DP_test, y_test, description=\"demographic parity with post-processed model on test data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing import ROCCurveBasedPostProcessing\n",
    "from copy import deepcopy\n",
    "\n",
    "post_processed_model_EO = ROCCurveBasedPostProcessing(fairness_unaware_model=deepcopy(fairness_unaware_model), disparity_metric=\"EqualizedOdds\", plot=True, seed=0)\n",
    "\n",
    "post_processed_model_EO.fit(X_train, y_train, aux_data=X_train.Sex)\n",
    "\n",
    "fairness_aware_predictions_EO_train = post_processed_model_EO.predict(X_train, group_data=X_train.Sex)\n",
    "fairness_aware_predictions_EO_test = post_processed_model_EO.predict(X_test, group_data=X_test.Sex)\n",
    "\n",
    "show_proportions(X_train, fairness_aware_predictions_EO_train, y_train, description=\"equalized odds with post-processed model on training data:\")\n",
    "show_proportions(X_test, fairness_aware_predictions_EO_test, y_test, description=\"equalized odds with post-processed model on test data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
