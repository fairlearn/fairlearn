{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and preparing the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the post processing algorithm we use the \"Adult Data Set\" from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/adult). The task there is to predict whether a person makes more or less than $50k based on just a few attributes such as age, sex, race, education, occupation, etc.\n",
    "\n",
    "To start, let's download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education_text', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
    "\n",
    "data = pd.read_csv('adult.data', names=columns)\n",
    "data_test = pd.read_csv('adult.test', names=columns, skiprows=[0])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to encode categorical attributes properly, there are a few additional steps to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def transform_data(data):\n",
    "    X = data[['age', 'education', 'capital_gain', 'capital_loss', 'hours_per_week']]\n",
    "    X = X.join((data.sex == \" Male\") * 1)\n",
    "    X = X.join(pd.get_dummies(data.workclass))\n",
    "    del X[' ?']\n",
    "    X = X.join(pd.get_dummies(data.occupation))\n",
    "    del X[' ?']\n",
    "    X = X.join(pd.get_dummies(data.relationship))\n",
    "    X = X.join(pd.get_dummies(data.race))\n",
    "    X = X.join(pd.get_dummies(data.native_country))\n",
    "    y = ((data.income == ' >50K') | (data.income == ' >50K.')) * 1\n",
    "    return X, y\n",
    "\n",
    "full_X, full_y = transform_data(pd.concat([data, data_test], ignore_index=True))\n",
    "X_train, y_train = full_X[:len(data)], full_y[:len(data)]\n",
    "X_test, y_test = full_X[len(data):], full_y[len(data):]\n",
    "X_test = X_test.set_index(pd.Index(list(range(len(X_test)))))\n",
    "y_test.index = pd.Index(list(range(len(y_test))))\n",
    "X_train.join(y_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a fairness-unaware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "fairness_unaware_model = LogisticRegression(solver='liblinear')\n",
    "fairness_unaware_model.fit(X_train, y_train)\n",
    "\n",
    "def show_proportions(X, y_pred, y=None, description=None):\n",
    "    male_indices = X.index[X.sex == 1]\n",
    "    female_indices = X.index[X.sex == 0]\n",
    "\n",
    "    males_earning_more_than_50k = sum(y_pred[male_indices])\n",
    "    females_earning_more_than_50k = sum(y_pred[female_indices])\n",
    "    print(\"\\n\" + description)\n",
    "    print(\"P[predicted income > 50k | male] = {}\".format(males_earning_more_than_50k/len(male_indices)))\n",
    "    print(\"P[predicted income > 50k | female] = {}\".format(females_earning_more_than_50k/len(female_indices)))\n",
    "    \n",
    "    if y is not None:\n",
    "        positive_male_indices = X.index[(X.sex == 1) & (y == 1)]\n",
    "        negative_male_indices = X.index[(X.sex == 1) & (y == 0)]\n",
    "        positive_female_indices = X.index[(X.sex == 0) & (y == 1)]\n",
    "        negative_female_indices = X.index[(X.sex == 0) & (y == 0)]\n",
    "        print(\"P[predicted income > 50k | male, income > 50k] = {}\".format(sum(y_pred[positive_male_indices])/len(positive_male_indices)))\n",
    "        print(\"P[predicted income > 50k | female, income > 50k] = {}\".format(sum(y_pred[positive_female_indices])/len(positive_female_indices)))\n",
    "        print(\"P[predicted income > 50k | male, income <= 50k] = {}\".format(sum(y_pred[negative_male_indices])/len(negative_male_indices)))\n",
    "        print(\"P[predicted income > 50k | female, income <= 50k] = {}\".format(sum(y_pred[negative_female_indices])/len(negative_female_indices)))\n",
    "    \n",
    "show_proportions(X_train, y_train, description=\"original training data:\")\n",
    "show_proportions(X_train, fairness_unaware_model.predict(X_train), y_train, description=\"fairness-unaware prediction on training data:\")\n",
    "show_proportions(X_test, y_test, description=\"original test data:\")\n",
    "show_proportions(X_test, fairness_unaware_model.predict(X_test), y_test, description=\"fairness-unaware prediction on test data:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a stark contrast in the predictions, with males being a lot more likely to be predicted to earn more than $\\$50k$, similar to the original training data. However, there's even a disparity between the subgroup of males and females that earn $\\$50k$ with 61.6% of males predicted to earn more than $\\$50k$, and only 48.5% of females. When considering only the samples labeled with $\\leq\\$50k$ males (9.8%) are more than five times as likely as females (1.9%) to be predicted to earn more than $\\$50k$. The test data shows a similar disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing the model to get a fair model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind post-processing is to alter the output of the fairness-unaware model to achieve fairness. The post-processing algorithm requires three input arguments:\n",
    "- the matrix of samples X\n",
    "- the vector of predictions y from the fairness-unaware model \n",
    "- the vector of protected attribute values A\n",
    "\n",
    "The goal is to make the output fair with respect to a disparity metric. Our post-processing algorithm uses one of\n",
    "- Demographic Parity (DP): $P[h(X)=\\hat{y} | A=a] = P[h(X)=\\hat{y}] \\qquad \\forall a, \\hat{y}$\n",
    "- Equalized Odds (EO): $P[h(X)=\\hat{y} | A=a, Y=y] = P[h(X)=\\hat{y}|Y=y] \\qquad \\forall a, \\hat{y}$\n",
    "\n",
    "where $h(X)$ is the prediction based on the input $X$, $\\hat{y}$ and $y$ are labels, and $a$ is a protected attribute value. In our example, we'd expect the post-processed model with DP to be balanced between sexes. EO does not make the same guarantees. Instead, it ensures that the parity between the subgroups of each sex with label 1 in the training set, and parity between the subgroups of each sex with label 0 in the training set. Applied to our scenario, this means that men and women with income over $\\$50,000$ are equally likely to have a predicted income over $\\$50,000$ (and therefore also equally likely to have a predicted income no higher than $\\$50,000$). Similarly, there is parity between men and women with income under $\\$50,000$, but we have no parity between the groups with different training labels. In mathematical terms:\n",
    "\n",
    "$$\n",
    "P[\\text{predicted income > 50k} | \\text{male, income > 50k}] = P[\\text{predicted income > 50k} | \\text{female, income > 50k}], \\text{e.g. } 0.95\\\\\n",
    "P[\\text{predicted income > 50k} | \\text{male, income <= 50k}] = P[\\text{predicted income > 50k} | \\text{female, income <= 50k}], \\text{e.g. } 0.15\n",
    "$$\n",
    "\n",
    "but that also means that men (and women) of different subgroup based on training labels don't necessarily have parity:\n",
    "\n",
    "$$\n",
    "P[\\text{predicted income > 50k} | \\text{male, income > 50k}] = 0.95 \\neq 0.15 = P[\\text{predicted income > 50k} | \\text{male, income <= 50k}]\n",
    "$$\n",
    "\n",
    "Assessing which disparity metric is indeed fair varies by application scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing.roc_curve_based_post_processing import ROCCurveBasedPostProcessing\n",
    "from fairlearn.metrics import DemographicParity\n",
    "from copy import deepcopy\n",
    "\n",
    "post_processed_model_DP = ROCCurveBasedPostProcessing(fairness_unaware_model=deepcopy(fairness_unaware_model), fairness_metric=DemographicParity(), plot=True, seed=0)\n",
    "\n",
    "post_processed_model_DP.fit(X_train.values, y_train.values, protected_attribute=X_train.sex.values)\n",
    "\n",
    "fairness_aware_predictions_DP_train = post_processed_model_DP.predict(X_train.values, protected_attribute=X_train.sex.values)\n",
    "fairness_aware_predictions_DP_test = post_processed_model_DP.predict(X_test.values, protected_attribute=X_test.sex.values)\n",
    "\n",
    "show_proportions(X_train, fairness_aware_predictions_DP_train, y_train, description=\"demographic parity with post-processed model on training data:\")\n",
    "show_proportions(X_test, fairness_aware_predictions_DP_test, y_test, description=\"demographic parity with post-processed model on test data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.post_processing.roc_curve_based_post_processing import ROCCurveBasedPostProcessing\n",
    "from fairlearn.metrics import EqualizedOdds\n",
    "from copy import deepcopy\n",
    "\n",
    "post_processed_model_EO = ROCCurveBasedPostProcessing(fairness_unaware_model=deepcopy(fairness_unaware_model), fairness_metric=EqualizedOdds(), plot=True, seed=0)\n",
    "\n",
    "post_processed_model_EO.fit(X_train.values, y_train.values, protected_attribute=X_train.sex.values)\n",
    "\n",
    "fairness_aware_predictions_EO_train = post_processed_model_EO.predict(X_train.values, protected_attribute=X_train.sex.values)\n",
    "fairness_aware_predictions_EO_test = post_processed_model_EO.predict(X_test.values, protected_attribute=X_test.sex.values)\n",
    "\n",
    "show_proportions(X_train, fairness_aware_predictions_EO_train, y_train, description=\"equalized odds with post-processed model on training data:\")\n",
    "show_proportions(X_test, fairness_aware_predictions_EO_test, y_test, description=\"equalized odds with post-processed model on test data:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
