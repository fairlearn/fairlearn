{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Metrics\n",
    "\n",
    "The `fairlearn` package contains algorithms which enable machine learning models to minimise disparity between groups. The `metrics` portion of the package provides the means required to verify that the mitigation algorithms are succeeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ungrouped Metrics\n",
    "\n",
    "At their simplest, metrics take a set of 'true' values $Y_{true}$ (from the input data) and predicted values $Y_{pred}$ (by applying the model to the input data), and use these to compute a measure. For example, the _recall_ or _true positive rate_ is given by\n",
    "\\begin{equation}\n",
    "P( Y_{pred}=1 | Y_{true}=1 )\n",
    "\\end{equation}\n",
    "That is, a measure of whether the model finds all the positive cases in the input data. The `scikit-learn` package implements this in [sklearn.metrics.recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html).\n",
    "\n",
    "Suppose we have the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1]\n",
    "Y_pred = [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the prediction is 1 in five of the ten cases where the true value is 1, so we expect the recall to be 0.0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.recall_score(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics with Grouping\n",
    "\n",
    "When considering fairness, each row of input data will have an associated group label $g \\in G$, and we will want to know how the metric behaves for each $g$. To help with this, `fairlearn` provides wrappers, which take an existing (ungrouped) metric function, and apply it to each group within a set of data.\n",
    "\n",
    "Suppose in addition to the $Y_{true}$ and $Y_{pred}$ above, we had the following set of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = ['d', 'a', 'c', 'b', 'b', 'c', 'c', 'c', 'b', 'd', 'c', 'a', 'b', 'd', 'c', 'c']\n",
    "\n",
    "df = pd.DataFrame({ 'Y_true': Y_true, 'Y_pred': Y_pred, 'group_data': group_data})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the groups 'a' and 'd' the recall is 0 (none of the true positives were identified), for 'b' the recall is 0.5 and for 'c' the recall is 0.75.\n",
    "\n",
    "The `fairlearn.metrics.metric_by_group` routine can calculate all of these for us. This takes as arguments an _ungrouped_ metric (such as `sklearn.metrics.recall_score`), the arrays $Y_{true}$ and $Y_{pred}$ and an array of group labels (and optionally, if the ungrouped metric supports it, an array of sample weights), and produces `GroupMetricResult` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairlearn.metrics as flm\n",
    "\n",
    "group_metrics = flm.metric_by_group(skm.recall_score, Y_true, Y_pred, group_data, sample_weight=None)\n",
    "\n",
    "print(\"Overall recall = \", group_metrics.overall)\n",
    "print(\"recall by groups = \", group_metrics.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the overall recall is the same as that calculated above in the Ungrouped Metric section, while the `by_group` dictionary matches the values we calculated by inspection from the table above.\n",
    "\n",
    "In addition to these basic scores, `metric_by_group` also records the maximum and minimum values of the metric, the groups for which these occurred, and also the difference and ratio between the maximum and minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min recall over groups = \", group_metrics.min_over_groups)\n",
    "print(\"occurred for groups: \", group_metrics.argmin_groups)\n",
    "print()\n",
    "print(\"max recall over groups = \", group_metrics.max_over_groups)\n",
    "print(\"occurred for groups: \", group_metrics.argmax_groups)\n",
    "print()\n",
    "print(\"range in recall = \", group_metrics.range_over_groups)\n",
    "print(\"ratio in recall = \", group_metrics.range_ratio_over_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Ungrouped Metrics\n",
    "\n",
    "To be used by `metric_by_group` the supplied Python function must take arguments of `y_true` and `y_pred`:\n",
    "```python\n",
    "my_metric_func(y_true, y_pred)\n",
    "```\n",
    "An additional argument of `sample_weight` is also supported:\n",
    "```python\n",
    "my_metric_with_weight(y_true, y_pred, sample_weight)\n",
    "```\n",
    "The `sample_weight` argument is always invoked by name, and _only_ if the user supplies a `sample_weight` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Wrapper\n",
    "\n",
    "Rather than require a call to `metric_by_group` each time, we also provide a function which turns an ungrouped metric into a grouped one. This is called `make_group_metric`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_recall_score = flm.make_group_metric(skm.recall_score)\n",
    "\n",
    "results = group_recall_score(Y_true, Y_pred, group_data)\n",
    "\n",
    "print(\"Overall recall = \", results.overall)\n",
    "print(\"recall by groups = \", results.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
