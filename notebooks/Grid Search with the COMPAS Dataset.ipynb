{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search with the COMPAS Dataset\n",
    "\n",
    "This notebook demonstrates the use of the grid search algorithm from `fairlearn` on the [COMPAS dataset from ProPublica](https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv). This dataset comes from the criminal justice system, with the labels (0 or 1) representing the two-year recidivism rate, specifically whether or not a given offender is re-arrested within two years (with a 0 representing no arrest). Models based on this dataset are used in bail decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Examining the Data\n",
    "\n",
    "We start by loading the dataset using the `tempeh` package (there may be some warnings, if you do not have `pytorch`, `keras` or `tensorflow` installed in your environment; these may be ignored). The data are already split into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'torch'. If you want to use pytorch with tempeh please install pytorch separately first.\n",
      "No modules named 'keras' and 'tensorflow'. If you want to use keras and tensorflow with tempeh please install keras and tensorflow separately first.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tempeh.configurations import datasets\n",
    "\n",
    "compas_dataset = datasets['compas']()\n",
    "X_train = pd.DataFrame(compas_dataset.X_train, columns=compas_dataset.features)\n",
    "y_train = pd.Series(compas_dataset.y_train.reshape(-1).astype(int), name=\"two_year_recid\")\n",
    "X_test = pd.DataFrame(compas_dataset.X_test, columns=compas_dataset.features)\n",
    "y_test = pd.Series(compas_dataset.y_test.reshape(-1).astype(int), name=\"two_year_recid\")\n",
    "sensitive_features_train = pd.Series(compas_dataset.race_train)\n",
    "sensitive_features_test = pd.Series(compas_dataset.race_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>age_cat_25 - 45</th>\n",
       "      <th>age_cat_Greater than 45</th>\n",
       "      <th>age_cat_Less than 25</th>\n",
       "      <th>c_charge_degree_F</th>\n",
       "      <th>c_charge_degree_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.340451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.010999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.129788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>42.487893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.513697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22.249714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>1.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>1.0</td>\n",
       "      <td>53.954471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3536 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex        age  juv_fel_count  juv_misd_count  juv_other_count  \\\n",
       "0     1.0  25.000000            0.0       -2.340451              1.0   \n",
       "1     0.0  26.000000            0.0        0.000000              0.0   \n",
       "2     1.0  21.000000            0.0        0.000000              0.0   \n",
       "3     1.0  29.129788            0.0        0.000000              0.0   \n",
       "4     1.0  42.487893            0.0        0.000000              0.0   \n",
       "...   ...        ...            ...             ...              ...   \n",
       "3531  1.0  33.000000            0.0        0.000000              0.0   \n",
       "3532  1.0  22.249714            0.0        0.000000              0.0   \n",
       "3533  1.0  35.000000            0.0        0.000000              0.0   \n",
       "3534  1.0  53.954471            0.0        0.000000              0.0   \n",
       "3535  0.0  31.000000            0.0        0.000000              0.0   \n",
       "\n",
       "      priors_count  age_cat_25 - 45  age_cat_Greater than 45  \\\n",
       "0       -15.010999              1.0                      0.0   \n",
       "1         0.000000              1.0                      0.0   \n",
       "2         0.000000              0.0                      0.0   \n",
       "3         6.000000              1.0                      0.0   \n",
       "4         7.513697              1.0                      0.0   \n",
       "...            ...              ...                      ...   \n",
       "3531      3.000000              1.0                      0.0   \n",
       "3532     23.000266              1.0                      0.0   \n",
       "3533      7.000000              1.0                      0.0   \n",
       "3534      0.000000              0.0                      1.0   \n",
       "3535      0.000000              1.0                      0.0   \n",
       "\n",
       "      age_cat_Less than 25  c_charge_degree_F  c_charge_degree_M  \n",
       "0                      0.0                0.0                1.0  \n",
       "1                      0.0                1.0                0.0  \n",
       "2                      1.0                1.0                0.0  \n",
       "3                      0.0                0.0                1.0  \n",
       "4                      0.0                1.0                0.0  \n",
       "...                    ...                ...                ...  \n",
       "3531                   0.0                1.0                0.0  \n",
       "3532                   0.0                0.0                1.0  \n",
       "3533                   0.0                1.0                0.0  \n",
       "3534                   0.0                1.0                0.0  \n",
       "3535                   0.0                1.0                0.0  \n",
       "\n",
       "[3536 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we treat Race as the sensitive attribute. The dataset has already been reduced to only have two values, \"African-American\" and \"Caucasian\", with approximately two thirds of the samples being African-American:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['African-American', 'Caucasian'], dtype=object),\n",
       " array([2147, 1389], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sensitive_features_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that race does not feature in the feature data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an unmitigated model\n",
    "\n",
    "Before attempting to mitigate any disparity, we should first train a model without regard to fairness. For simplicity, we will use a logistic regression model, as implemented by `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unconstrained_predictor = LogisticRegression(solver='liblinear')\n",
    "unconstrained_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained, we can examine it in the Fairness Dashboard. There are a number of sections which we can examine.\n",
    "\n",
    "First is the Accuracy - the fraction of cases where the model gave the right answer. While the overall accuracy is a little over 66%, this number hides some complexity. While both subgroups had a similar overall accuracy, we can see that African-Americans had a much higher overestimation error (i.e. the model predicts that they will be rearrested when they were not) while Caucasians have a much higher underestimation error (i.e. the model predicts that they will not rearrested, but they were).\n",
    "\n",
    "If we instead look at the Recall (which measures model's ability to find all of the positive samples), we can see a much lower score for Caucasians than African-Americans. The story for the Specificity (which measures the ability of a model to find all of the negative samples - in this case, thoses where there was no rearrest) is reversed, with Caucasians having a specificity of nearly 80%, but African-Americans only showing a specificity score of about 65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8994b1aa20a4c92bf22cb3e6fa62020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget.fairlearnDashboard.FairlearnDashboard at 0x2e1144040b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "\n",
    "predicted_ys = [unconstrained_predictor.predict(X_test).tolist()]\n",
    "sensitive_features_mapped = list(map(lambda x: [x], sensitive_features_test.values))\n",
    "\n",
    "FairlearnDashboard(sensitive_features=sensitive_features_mapped,\n",
    "                   true_y=y_test.values,\n",
    "                   predicted_ys=predicted_ys,\n",
    "                   class_names=None,\n",
    "                   feature_names=[\"Race\"],\n",
    "                   is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Disparity Constraint\n",
    "\n",
    "Before we can try to reduce disparity, we must first ask what the relevant constraint on the disparity should be. There are two options currently in `fairlearn` - Demographic Parity and Equalized Odds. While `fairlearn` produce models which reduce violation of the specified constraint, that does not mean that the models are *fairer* in the broader societal context.\n",
    "\n",
    "In the following, we use $A$ for the sensitive attribute, $Y$ for the true values and $\\hat{Y}$ for the predicted values. Since we have a binary classification problem, $Y , \\hat{Y} \\in \\{ 0, 1 \\}$.\n",
    "\n",
    "Demographic Parity requires that $P( \\hat{Y} | A ) = P(\\hat{Y})$. That is, each subgroup (African-Americans and Caucasians in this case) should be equally likely to get a positive prediction (which in this example means \"rearrested\").\n",
    "\n",
    "Equalized Odds requires that $P( \\hat{Y} | A, Y ) = P( \\hat{Y} | Y)$, which corresponds to two separate equations for the two possible values of $Y$. For the case $Y=1$, this is equivalent to equalizing the true positive rates (also known as \"Recall\") across groups. In the $Y=0$ case, this is equivalent to equalizing the false positive rates (also known as \"Fall-Out\") across groups.\n",
    "\n",
    "If we are using our model to make bail decisions, we want to minimise the number of offences commited when out on bail. We use the rearrest feature as a proxy for this (note that there are a number of issues with doing so). Demographic Parity does not make much sense in this case - what that will do is equalise the chances of predicting a rearrest. In contrast, Equalized Odds does - we will be aiming to predict a rearrest correctly at equal rates for African-Americans and Caucasians, and also predict rearrests which would not actually have occurred at equal rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
