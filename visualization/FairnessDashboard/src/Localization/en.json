{
    "loremIpsum": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",
    "defaultClassNames": "Class {0}",
    "defaultFeatureNames": "Sensitive feature {0}",
    "defaultSingleFeatureName": "Sensitive feature",
    "defaultCustomMetricName": "Custom metric {0}",
    "accuracyTab": "Fairness in Performance",
    "opportunityTab": "Fairness in Opportunity",
    "modelComparisonTab": "Model comparison",
    "tableTab": "Detail View",
    "dataSpecifications": "Data statistics",
    "attributes": "Attributes",
    "singleAttributeCount": "1 sensitive feature",
    "attributesCount": "{0} sensitive features",
    "instanceCount": "{0} instances",
    "close": "Close",
    "calculating": "Calculating...",
    "accuracyMetric": "Performance metric",
    "errorOnInputs": "Error with input. Sensitive features must be categorical values at this time. Please map values to binned categories and retry.",
    "Accuracy": {
        "header": "How do you want to measure performance?",
        "modelMakes": "model makes",
        "modelsMake": "models make",
        "body": "Your data contains {0} labels and your {2} {1} predictions. Based on that information, we recommend the following metrics. Please select one metric from the list.",
        "binaryClassifier": "binary classifier",
        "probabalisticRegressor": "probit regressor",
        "regressor": "regressor",
        "binary": "binary",
        "continuous": "continuous"
    },
    "Parity": {
        "header": "Fairness measured in terms of disparity",
        "body": "Disparity metrics quantify variation of your model's behavior across selected features. There are two kinds of disparity metrics: more to come...."
    },
    "Header": {
        "title": "Fairlearn",
        "documentation": "Documentation"
    },
    "Footer": {
        "back": "Back",
        "next": "Next"
    },
    "Intro": {
        "welcome": "Welcome to the",
        "fairlearnDashboard": "Fairlearn dashboard",
        "introBody": "The Fairlearn dashboard enables you to assess tradeoffs between performance and fairness of your models",
        "explanatoryStep": "To set up the assessment, you need to specify a sensitive feature and a performance metric.",
        "getStarted": "Get started",
        "features": "Sensitive features",
        "featuresInfo": "Sensitive features are used to split your data into groups. Fairness of your model across these groups is measured by disparity metrics. Disparity metrics quantify how much your model's behavior varies across these groups.",
        "accuracy": "Performance metric",
        "accuracyInfo": "Performance metrics are used to evaluate the overall quality of your model as well as the quality of your model in each group. The difference between the extreme values of the performance metric across the groups is reported as the disparity in performance."
    },
    "ModelComparison": {
        "title": "Model comparison",
        "howToRead": "How to read this chart",
        "lower": "lower",
        "higher": "higher",
        "howToReadText": "This chart represents each of the {0} models as a selectable point. The x-axis represents {1}, with {2} being better. The y-axis represents disparity, with lower being better.",
        "insights": "Insights",
        "insightsText1": "The chart shows {0} and disparity of {1} models.",
        "insightsText2": "{0} ranges from {1} to {2}. The disparity ranges from {3} to {4}.",
        "insightsText3": "The most accurate model achieves {0} of {1} and a disparity of {2}.",
        "insightsText4": "The lowest-disparity model achieves {0} of {1} and a disparity of {2}.",
        "disparityInOutcomes": "Disparity in predictions",
        "disparityInAccuracy": "Disparity in {0}",
        "howToMeasureDisparity": "How should disparity be measured?"
    },
    "Report": {
        "modelName": "Model {0}",
        "title": "Disparity in performance",
        "globalAccuracyText": "Is the overall {0}",
        "accuracyDisparityText": "Is the disparity in {0}",
        "editConfiguration": "Edit configuration",
        "backToComparisons": "Multimodel view",
        "outcomesTitle": "Disparity in predictions",
        "minTag": "Min",
        "maxTag": "Max",
        "groupLabel": "Subgroup",
        "underestimationError": "Underprediction",
        "underpredictionExplanation": "(predicted = 0, true = 1)",
        "overpredictionExplanation": "(predicted = 1, true = 0)",
        "overestimationError": "Overprediction",
        "classificationOutcomesHowToRead": "The bar chart shows the selection rate in each group, meaning the fraction of points classified as 1.",
        "regressionOutcomesHowToRead": "Box plots show the distribution of predictions in each group. Individual data points are overlaid on top.",
        "classificationAccuracyHowToRead1": "The bar chart shows the distribution of errors in each group.",
        "classificationAccuracyHowToRead2": "Errors are split into overprediction errors (predicting 1 when the true label is 0), and underprediction errors (predicting 0 when the true label is 1).",
        "classificationAccuracyHowToRead3": "The reported rates are obtained by dividing the number of errors by the overall group size.",
        "probabilityAccuracyHowToRead1": "The bar chart shows mean absolute error in each group, split into overprediction and underprediction.",
        "probabilityAccuracyHowToRead2": "On each example, we measure the difference between the prediction and the label. If it is positive, we call it overprediction and if it is negative, we call it underprediction.",
        "probabilityAccuracyHowToRead3": "We report the sum of overprediction errors and the sum of underprediction errors divided by the overall group size.",
        "regressionAccuracyHowToRead": "Error is the difference between the prediction and the label. Box plots show the distribution of errors in each group. Individual data points are overlaid on top.",
        "distributionOfPredictions": "Distribution of predictions",
        "distributionOfErrors": "Distribution of errors",
        "tooltipPrediction": "Prediction: {0}",
        "tooltipError": "Error: {0}"
    },
    "Feature": {
        "header": "Along which features would you like to evaluate your model's fairness?",
        "body": "Fairness is evaluated in terms of disparities in your model's behavior. We will split your data according to values of each selected feature, and evaluate how your model's performance metric and predictions differ across these splits.",
        "learnMore": "Learn more",
        "summaryCategoricalCount": "This feature has {0} unique values",
        "summaryNumericCount": "This numeric feature ranges from {0} to {1}, and is grouped into {2} bins.",
        "showCategories": "Show all",
        "hideCategories": "Collapse",
        "categoriesOverflow": "   and {0} additional categories",
        "editBinning": "Edit groups",
        "subgroups": "Subgroups"
    },
    "Metrics": {
        "accuracyScore": "Accuracy",
        "precisionScore": "Precision",
        "recallScore": "Recall",
        "zeroOneLoss": "Zero-one loss",
        "specificityScore": "Specificity score",
        "missRate": "Miss rate",
        "falloutRate": "Fallout rate",
        "maxError": "Max error",
        "meanAbsoluteError": "Mean absolute error",
        "meanSquaredError": " MSE (mean squared error)",
        "meanSquaredLogError": "Mean squared log error",
        "medianAbsoluteError": "Median absolute error",
        "average": "Average prediction",
        "selectionRate": "Selection rate",
        "overprediction": "Overprediction",
        "underprediction": "Underprediction",
        "r2_score": "R-squared score",
        "rms_error": "RMSE (root mean squared error)",
        "auc": "Area under ROC curve",
        "balancedRootMeanSquaredError": "Balanced RMSE",
        "balancedAccuracy": "Balanced accuracy",
        "f1Score": "F1-Score",
        "logLoss": "Log Loss",
        "accuracyDescription": "The fraction of data points classified correctly.",
        "precisionDescription": "The fraction of data points classified correctly among those classified as 1.",
        "recallDescription": "The fraction of data points classified correctly among those whose true label is 1. Alternative names: true positive rate, sensitivity.",
        "rmseDescription": "Square root of the average of squared errors.",
        "mseDescription": "The average of squared errors.",
        "meanAbsoluteErrorDescription": "The average of absolute values of errors. More robust to outliers than MSE.",
        "r2Description": "The fraction of variance in the labels explained by the model.",
        "aucDescription": "The quality of the predictions, viewed as scores, in separating positive examples from negative examples.",
        "balancedRMSEDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
        "balancedAccuracyDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
        "f1ScoreDescription": "F1-Score is the harmonic mean of precision and recall."
    },
    "BinDialog": {
        "header": "Configure bins",
        "makeCategorical": "Treat as categorical",
        "save": "Save",
        "cancel": "Cancel",
        "numberOfBins": "Number of bins:",
        "categoryHeader": "Bin values:"
    }
}