{
    "loremIpsum": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",
    "defaultClassNames": "Class {0}",
    "defaultFeatureNames": "Feature {0}",
    "accuracyTab": "Fairness in Accuracy",
    "opportunityTab": "Fairness in Opportunity",
    "modelComparisonTab": "Model comparison",
    "tableTab": "Detail View",
    "dataSpecifications": "Data statistics",
    "attributes": "Attributes",
    "singleAttributeCount": "1 sensitive feature",
    "attributesCount": "{0} sensitive feature",
    "instanceCount": "{0} instances",
    "close": "Close",
    "calculating": "Calculating...",
    "Accuracy": {
        "header": "How do you want to measure accuracy?",
        "body": "We have determined that your model is a {0}. Based on this, we recommend the following accuracy metrics:",
        "binaryClassifier": "binary classifier",
        "probabalisticRegressor": "probit regressor",
        "regressor": "regressor"
    },
    "Parity": {
        "header": "Fairness measured in terms of disparity",
        "body": "Disparity metrics quantify variation of your model's behavior across selected features. There are two kinds of disparity metrics: more to come...."
    },
    "Header": {
        "title": "fairlearn",
        "documentation": "Documentation"
    },
    "Footer": {
        "back": "Back",
        "next": "Next"
    },
    "Intro": {
        "welcome": "Welcome to the",
        "explanatoryStep": "Setting up your fairness assessment is easy",
        "getStarted": "Get started",
        "features": "Sensitive features",
        "accuracy": "Accuracy"
    },
    "ModelComparison": {
        "title": "Model Comparison",
        "howToRead": "How to read this chart",
        "lower": "lower",
        "higher": "higher",
        "howToReadText": "This chart represents each of the {0} models as a selectable point. The x-axis represents {1}, with {2} being better. The y-axis represents disparity, with lower being better.",
        "insights": "Insights",
        "insightsText1": "The chart shows {0} and disparity of {1} models.",
        "insightsText2": "{0} ranges from {1} to {2}. The disparity ranges from {3} to {4}.",
        "insightsText3": "The most accurate model achieves {0} of {1} and a disparity of {2}.",
        "insightsText4": "The lowest-disparity model achieves {0} of {1} and a disparity of {2}.",
        "disparityInOutcomes": "Disparity in predictions",
        "disparityInAccuracy": "Disparity in {0}",
        "howToMeasureDisparity": "How should disparity be measured?"
    },
    "Report": {
        "modelName": "Model {0}",
        "title": "Disparity in accuracy",
        "globalAccuracyText": "Is the overall {0}",
        "accuracyDisparityText": "Is the disparity in {0}",
        "editConfiguration": "Edit configuration",
        "backToComparisons": "Multimodel view",
        "outcomesTitle": "Disparity in predictions",
        "minTag": "Min",
        "maxTag": "Max",
        "groupLabel": "Subgroup",
        "underestimationError": "Underestimation error",
        "overestimationError": "Overestimation error",
        "classificationOutcomesHowToRead": "This chart shows the selection rate, the number of points in each group that were classified as 1 divided by the size of the group",
        "regressionOutcomesHowToRead": "This chart shows the distribution of predictions for each group. The individual points are overlaid on the box plot.",
        "classificationAccuracyHowToRead": "Underestimation error rate is the number of points where the model predicted 0 when the true value was 1, divided by the group size. Overestimation error rate is the number of points where the model predicted 1 when the true value was 0, divided by the group size.",
        "probabilityAccuracyHowToRead": "Underestimation error rate is the sum of all negative errors within a group divided by the group size. Overestimation error rate is the sum of all positive errors in a group divided by the group size.",
        "regressionAccuracyHowToRead": "This chart shows the error distribution for each group. The error is the predicted value minus the true value."
    },
    "Feature": {
        "header": "Along which features would you like to evaluate your model's fairness?",
        "body": "Fairness is evaluated in terms of disparities in your model's behavior. We will split your data according to values of each selected feature, and evaluate how your model's accuracy and predictions differ across these splits.",
        "learnMore": "Learn more",
        "summaryCategoricalCount": "This feature has {0} unique values",
        "showCategories": "Show values",
        "hideCategories": "Hide categories",
        "categoriesOverflow": "   and {0} additional categories"
    },
    "Metrics": {
        "accuracyScore": "Accuracy",
        "precisionScore": "Precision",
        "recallScore": "Recall",
        "zeroOneLoss": "Zero-one loss",
        "specificityScore": "Specificity score",
        "missRate": "Miss rate",
        "falloutRate": "Fallout rate",
        "maxError": "Max error",
        "meanAbsoluteError": "Mean absolute error",
        "meanSquaredError": "Mean squared error",
        "meanSquaredLogError": "Mean squared log error",
        "medianAbsoluteError": "Median absolute error",
        "average": "Average prediction",
        "selectionRate": "Selection rate",
        "overprediction": "Overprediction",
        "underprediction": "Underprediction",
        "balancedRootMeanSquaredError": "Balanced root mean squared error"
    }
}