@inproceedings{agarwal2018reductions,
  author    = {Alekh Agarwal and
               Alina Beygelzimer and
               Miroslav Dudík and
               John Langford and
               Hanna M. Wallach},
  title     = {A Reductions Approach to Fair Classification},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {60--69},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/agarwal18a.html}
}

@inproceedings{agarwal2019fair,
  author    = {Alekh Agarwal and
               Miroslav Dudík and
               Zhiwei Steven Wu},
  title     = {Fair Regression: Quantitative Definitions and Reduction-Based Algorithms},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {120--129},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/agarwal19d.html}
}

@book{barocas2019fairness,
  title = {Fairness and Machine Learning},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org/}},
  year = {2019}
}


@inproceedings{hardt2016equality,
  author    = {Moritz Hardt and
               Eric Price and
               Nati Srebro},
  title     = {Equality of Opportunity in Supervised Learning},
  booktitle = {{NeurIPS}},
  pages     = {3315--3323},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html}
}

@inproceedings{selbst2019fairness,
author = {Selbst, Andrew D. and {boyd}, {danah} and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
title = {Fairness and Abstraction in Sociotechnical Systems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287598},
doi = {10.1145/3287560.3287598},
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {59–68},
numpages = {10},
keywords = {Fairness-aware Machine Learning, Sociotechnical Systems, Interdisciplinary},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{jacobs2021measurement,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {construct reliability, measurement, fairness, construct validity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{obermeyer2019dissecting,
author = {Ziad Obermeyer  and Brian Powers  and Christine Vogeli  and Sendhil Mullainathan },
title = {Dissecting racial bias in an algorithm used to manage the health of populations},
journal = {Science},
volume = {366},
number = {6464},
pages = {447-453},
year = {2019},
doi = {10.1126/science.aax2342},
URL = {https://www.science.org/doi/abs/10.1126/science.aax2342},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aax2342},
abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.}}

@article{crenshaw1991intersectionality,
author = {Kimberlé Crenshaw},
title = {Mapping the Margins: Intersectionality, Identity Politics, and Violence against Women of Color},
journal = {Stanford Law Review},
volume = {43},
number = {6},
pages = {1241-1299},
year = {1991},
doi = {https://doi.org/10.2307/1229039},
URL = {https://www.jstor.org/stable/1229039},
eprint = {https://www.jstor.org/stable/1229039}
}

@misc{Xiang2019legalcompatibility,
  doi = {10.48550/ARXIV.1912.00761},
  url = {https://arxiv.org/abs/1912.00761},
  author = {Xiang, Alice and Raji, Inioluwa Deborah},
  keywords = {Computers and Society (cs.CY), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {On the Legal Compatibility of Fairness Definitions},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{watkins2022fourfifths,
  doi = {10.48550/ARXIV.2202.09519},
  url = {https://arxiv.org/abs/2202.09519},
  author = {Watkins, Elizabeth Anne and McKenna, Michael and Chen, Jiahao},
  keywords = {Computers and Society (cs.CY), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Logic in Computer Science (cs.LO), FOS: Computer and information sciences, FOS: Computer and information sciences, K.4; K.5; F.4; I.2, 68T27, 03B70},
  title = {The four-fifths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bickel1975biasinadmissions,
author = {Bickel, P.J. and Hammel, E.A. and and O'Connell, E.W.},
title = {Sex Bias in Graduate Admissions: Data from Berkeley},
journal = {Science},
volume = {187},
number = {4175},
pages = {398-404},
year = {1975},
doi = {10.1126%2Fscience.187.4175.398},
URL = {https://doi.org/10.1126%2Fscience.187.4175.398}
}

@article{strack2014impact,
author = {Strack, Beata and Deshazo, Jonathan and Gennings, Chris and Olmo Ortiz, Juan Luis and Ventura, Sebastian and Cios, Krzysztof and Clore, John},
year = {2014},
month = {04},
pages = {781670},
title = {Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records},
volume = {2014},
journal = {BioMed research international},
doi = {10.1155/2014/781670}
}

@misc{strack2014diabetes,
author = {Strack, Beata and Deshazo, Jonathan and Gennings, Chris and Olmo Ortiz, Juan Luis and Ventura, Sebastian and Cios, Krzysztof and Clore, John},
year = {2014},
month = {05},
title = {Diabetes 130-US hospitals for years 1999-2008 Data Set},
URL = {https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008}
}
