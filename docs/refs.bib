@inproceedings{agarwal2018reductions,
  author    = {Alekh Agarwal and
               Alina Beygelzimer and
               Miroslav Dudík and
               John Langford and
               Hanna M. Wallach},
  title     = {A Reductions Approach to Fair Classification},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {60--69},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/agarwal18a.html}
}

@inproceedings{agarwal2019fair,
  author    = {Alekh Agarwal and
               Miroslav Dudík and
               Zhiwei Steven Wu},
  title     = {Fair Regression: Quantitative Definitions and Reduction-Based Algorithms},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {120--129},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/agarwal19d.html}
}


@inproceedings{al2018comparative,
  title={A comparative study of different curve fitting algorithms in artificial neural network using housing dataset},
  author={Al Bataineh, Ali and Kaur, Devinder},
  booktitle={Naecon 2018-ieee national aerospace and electronics conference},
  pages={174--178},
  year={2018},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/8556738}
}


@article{barocas2016big,
  title={Big data's disparate impact},
  author={Barocas, Solon and Selbst, Andrew D},
  journal={California law review},
  pages={671--732},
  year={2016},
  url={https://www.jstor.org/stable/24758720}
}


@book{barocas2019fairness,
  title = {Fairness and Machine Learning},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  url = {http://www.fairmlbook.org/},
  year = {2019}
}


@article{bird2020fairlearn,
  title={Fairlearn: A toolkit for assessing and improving fairness in AI},
  author={Bird, Sarah and Dud{\'\i}k, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
  journal={Microsoft, Tech. Rep. MSR-TR-2020-32},
  year={2020},
  url={https://www.microsoft.com/en-us/research/uploads/prod/2020/05/Fairlearn_whitepaper.pdf}
}


@book{broussard2018artificial,
  title={Artificial unintelligence: How computers misunderstand the world},
  author={Broussard, Meredith},
  year={2018},
  publisher={MIT Press},
  url={https://mitpress.mit.edu/books/artificial-unintelligence}
}


@article{cortbettdavies2022measure,
  title = {The Measure and Mismeasure of Fairness},
  author = {Sam Corbett-Davies and Johann D. Gaebler and Hamed Nilforoshan and Ravi Shroff and Sharad Goel},
  journal = {Working paper},
  year = {2022},
  url = {https://5harad.com/papers/fair-ml.pdf}
}


@inproceedings{dwork2012awareness,
  title={Fairness through awareness},
  author={Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle={{ITCS}},
  pages={214--226},
  year={2012},
  url={https://dl.acm.org/doi/abs/10.1145/2090236.2090255}
}


@inproceedings{fazelpour2020algorithmic,
  title={Algorithmic fairness from a non-ideal perspective},
  author={Fazelpour, Sina and Lipton, Zachary C},
  booktitle={{AIES}},
  pages={57--63},
  year={2020},
  url={https://dl.acm.org/doi/10.1145/3375627.3375828}
}


@inproceedings{hardt2016equality,
  author    = {Moritz Hardt and
               Eric Price and
               Nati Srebro},
  title     = {Equality of Opportunity in Supervised Learning},
  booktitle = {{NeurIPS}},
  pages     = {3315--3323},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html}
}


@article{harrison1978hedonic,
  title={Hedonic housing prices and the demand for clean air},
  author={Harrison, Jr, David and Rubinfeld, Daniel L},
  journal={Journal of environmental economics and management},
  volume={5},
  number={1},
  pages={81--102},
  year={1978},
  publisher={Elsevier},
  url={https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf?sequence=1&isAllowed=y}
}


@inproceedings{madaio2020codesigning,
  title={Co-designing checklists to understand organizational challenges and opportunities around fairness in AI},
  author={Madaio, Michael A and Stark, Luke and Wortman Vaughan, Jennifer and Wallach, Hanna},
  booktitle={{ACM CHI}},
  pages={1--14},
  year={2020},
  url={https://dl.acm.org/doi/10.1145/3313831.3376445}
}


@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/10.1145/3457607}
}


@incollection{noble2018algorithms,
  title={Algorithms of oppression},
  author={Noble, Safiya Umoja},
  booktitle={Algorithms of oppression},
  year={2018},
  publisher={New York University Press},
  url={https://nyupress.org/9781479837243/algorithms-of-oppression/}
}


@book{oneil2017weapons,
  title={Weapons of math destruction: How big data increases inequality and threatens democracy},
  author={O'Neil, Cathy},
  year={2017},
  publisher={Crown},
  url={https://www.penguinrandomhouse.com/books/241363/weapons-of-math-destruction-by-cathy-oneil/}
}


@inproceedings{selbst2019fairness,
author = {Selbst, Andrew D. and {boyd}, {danah} and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
title = {Fairness and Abstraction in Sociotechnical Systems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://dl.acm.org/doi/10.1145/3287560.3287598},
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {59–68},
numpages = {10},
keywords = {Fairness-aware Machine Learning, Sociotechnical Systems, Interdisciplinary},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{jacobs2021measurement,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {construct reliability, measurement, fairness, construct validity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@misc{kain1975housing,
  title={Housing Markets and Racial Discrimination: A Microeconomic Analysis},
  author={Kain, John F. and Quigley, John M.},
  year={1975},
  url={https://www.nber.org/books-and-chapters/housing-markets-and-racial-discrimination-microeconomic-analysis}
}

@article{obermeyer2019dissecting,
author = {Ziad Obermeyer  and Brian Powers  and Christine Vogeli  and Sendhil Mullainathan },
title = {Dissecting racial bias in an algorithm used to manage the health of populations},
journal = {Science},
volume = {366},
number = {6464},
pages = {447-453},
year = {2019},
doi = {10.1126/science.aax2342},
URL = {https://www.science.org/doi/abs/10.1126/science.aax2342},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aax2342},
abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.}}

@article{crenshaw1991intersectionality,
author = {Kimberlé Crenshaw},
title = {Mapping the Margins: Intersectionality, Identity Politics, and Violence against Women of Color},
journal = {Stanford Law Review},
volume = {43},
number = {6},
pages = {1241-1299},
year = {1991},
doi = {https://doi.org/10.2307/1229039},
URL = {https://www.jstor.org/stable/1229039},
eprint = {https://www.jstor.org/stable/1229039}
}

@inproceedings{zhang2018mitigating,
  title={Mitigating unwanted biases with adversarial learning},
  author={Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={335--340},
  year={2018}
}

@misc{Xiang2019legalcompatibility,
  doi = {10.48550/ARXIV.1912.00761},
  url = {https://arxiv.org/abs/1912.00761},
  author = {Xiang, Alice and Raji, Inioluwa Deborah},
  keywords = {Computers and Society (cs.CY), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {On the Legal Compatibility of Fairness Definitions},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{watkins2022fourfifths,
  doi = {10.48550/ARXIV.2202.09519},
  url = {https://arxiv.org/abs/2202.09519},
  author = {Watkins, Elizabeth Anne and McKenna, Michael and Chen, Jiahao},
  keywords = {Computers and Society (cs.CY), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Logic in Computer Science (cs.LO), FOS: Computer and information sciences, FOS: Computer and information sciences, K.4; K.5; F.4; I.2, 68T27, 03B70},
  title = {The four-fifths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bickel1975biasinadmissions,
author = {Bickel, P.J. and Hammel, E.A. and and O'Connell, E.W.},
title = {Sex Bias in Graduate Admissions: Data from Berkeley},
journal = {Science},
volume = {187},
number = {4175},
pages = {398-404},
year = {1975},
doi = {10.1126%2Fscience.187.4175.398},
URL = {https://doi.org/10.1126%2Fscience.187.4175.398}
}

@article{strack2014impact,
author = {Strack, Beata and Deshazo, Jonathan and Gennings, Chris and Olmo Ortiz, Juan Luis and Ventura, Sebastian and Cios, Krzysztof and Clore, John},
year = {2014},
month = {04},
pages = {781670},
title = {Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records},
volume = {2014},
journal = {BioMed research international},
doi = {10.1155/2014/781670}
}

@misc{strack2014diabetes,
author = {Strack, Beata and Deshazo, Jonathan and Gennings, Chris and Olmo Ortiz, Juan Luis and Ventura, Sebastian and Cios, Krzysztof and Clore, John},
year = {2014},
month = {05},
title = {Diabetes 130-US hospitals for years 1999-2008 Data Set},
URL = {https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008}
}


@book{umojanoble2018algorithmsoppression,
  title = {Algorithms of Oppression},
  author = {Safiya {Umoja Noble}},
  publisher = {NYU Press},
  note = {\url{http://algorithmsofoppression.com/}},
  year = {2018}
}

@inproceedings{barocas2017problem,
  title={The problem with bias: Allocative versus representational harms in machine learning},
  author={Barocas, Solon and Crawford, Kate and Shapiro, Aaron and Wallach, Hanna},
  booktitle={9th Annual conference of the special interest group for computing, information and society},
  year={2017}
}

@inproceedings{shahhosseini2020optimizing,
  title={Optimizing ensemble weights for machine learning models: A case study for housing price prediction},
  author={Shahhosseini, Mohsen and Hu, Guiping and Pham, Hieu},
  booktitle={Smart Service Systems, Operations Management, and Analytics: Proceedings of the 2019 INFORMS International Conference on Service Science},
  pages={87--97},
  year={2020},
  organization={Springer},
  url={https://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=1187&context=imse_conf}
}

@article{tipping1999relevance,
  title={The relevance vector machine},
  author={Tipping, Michael},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999},
  url={https://proceedings.neurips.cc/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Paper.pdf}
}

@misc{scikitlearn2022ames,
  title={The Ames housing dataset},
  author={Scikit-Learn developers},
  year={2022},
  url={https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_ames_housing.html}
}

@misc{scikitlego2019fairness,
  title={Fairness},
  author={Warmerdam, Vincent and Brouns, Matthijs and Scikit-Lego contributors},
  year={2019},
  url={https://scikit-lego.netlify.app/fairness.html}
}

@misc{carlisle2019racist,
  title={racist data destruction?},
  author={M Carlisle},
  year={2019},
  url={https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8}
}

@misc{uscensusbureaumetropolitan,
  title={Metropolitan Areas},
  author={United States Census Bureau},
  url={https://www.census.gov/history/www/programs/geography/metropolitan_areas.html}
}

@misc{nedlund2019apple,
  title={Apple Card is accused of gender bias. Here’s how that can happen},
  author={Evelina Nedlund},
  url={https://edition.cnn.com/2019/11/12/business/apple-card-gender-bias/index.html},
  year={2019}
}

@inproceedings{chen2019fairness,
  title={Fairness under unawareness: Assessing disparity when protected class is unobserved},
  author={Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={339--348},
  year={2019},
  url={https://dl.acm.org/doi/abs/10.1145/3287560.3287594}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019},
  url={https://dl.acm.org/doi/10.1145/3287560.3287596}
}

@misc{dudik2020assessing,
  title={Assessing and mitigating unfairness in credit models with the Fairlearn toolkit},
  author={Dudík, Miroslav and Chen, William and Barocas, Solon and Inchiosa, Mario and Lewins, Nick and Oprescu, Miruna and Qiao, Joy and Sameki, Mehrnoosh and Schlener, Mario and Tuo, Jason and Wallach, Hanna},
  url={https://www.microsoft.com/en-us/research/uploads/prod/2020/09/Fairlearn-EY_WhitePaper-2020-09-22.pdf},
  year={2020}
}

@misc{peyton2020redlining,
  title={Redlining in America: How a history of housing discrimination endures},
  author={Nellie Peyton},
  year={2020},
  url={https://www.context.news/socioeconomic-inclusion/redlining-in-america-how-housing-discrimination-endures}
}

@misc{jan2018redlining,
  author={Tracy Jan},
  year={2018},
  title={Redlining was banned 50 years ago. It's still hurting minorities today.},
  url={https://www.washingtonpost.com/news/wonk/wp/2018/03/28/redlining-was-banned-50-years-ago-its-still-hurting-minorities-today}
}

@article{yeh2009comparisons,
  title={The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients},
  author={Yeh, I-Cheng and Lien, Che-hui},
  journal={Expert systems with applications},
  volume={36},
  number={2},
  pages={2473--2480},
  year={2009},
  publisher={Elsevier},
  url={https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients}
}

@misc{uscode2011title15chapter41subchapteriv,
  title={United States Code 2011 Edition - Title 15 Commerce and Trade - Chapter 41 Consumer Credit Protection - Subchapter IV—Equal Credit Opportunity},
  url={https://www.govinfo.gov/content/pkg/USCODE-2011-title15/html/USCODE-2011-title15-chap41-subchapIV.htm}
}